#!/usr/bin/env python
# coding: utf-8

# ì¤‘ë³µëœ ë°ì´í„°ë¥¼ ì°¾ì•„ ì œê±°í•  ìˆ˜ ìˆê³ , ê²°ì¸¡ì¹˜(missing data)ë¥¼ ì œê±°í•˜ê±°ë‚˜ ì±„ì›Œ ë„£ì„ ìˆ˜ ìˆë‹¤.
# ë°ì´í„°ë¥¼ ì •ê·œí™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
# ì´ìƒì¹˜(outlier)ë¥¼ ì°¾ê³ , ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì›-í•« ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.
# ì—°ì†ì ì¸ ë°ì´í„°ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ  ë²”ì£¼í˜• ë°ì´í„°ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.
# 
# <ë°°ìš¸ ë‚´ìš©>
# 
# ê²°ì¸¡ì¹˜(Missing Data)
# [ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•]
# i) ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë°ì´í„° ì œê±°
# ii)ê²°ì¸¡ì¹˜ë¥¼ ì–´ë–¤ ê°’ìœ¼ë¡œ ëŒ€ì²´(ë°ì´í„°ë§ˆë‹¤ íŠ¹ì„± ë°˜ì˜í•˜ì—¬ í•´ê²°)
#  i-1)íŠ¹ì •ê°’ ì§€ì •. ê²°ì¸¡ì¹˜ê°€ ë§ì€ ê²½ìš°( ëª¨ë‘ ê°™ì€ ê°’ ëŒ€ì²´-->ê°’ì´ ê°ì†Œ)
#  ii-2)í‰ê· ,ì¤‘ì•™ê°’ ëŒ€ì²´. ì‹¤ì œë³´ë‹¤ ì‘ì•„ì§ˆ ìˆ˜ ìˆìŒ
#  ii-3)ì˜ˆì¸¡ê°’ìœ¼ë¡œ ëŒ€ì²´
#  ii-4)ì‹œê³„ì—´ íŠ¹ì„± ë°ì´í„°, ê²°ì¸¡ì¹˜ ëŒ€ì²´!
# 
# ì¤‘ë³µëœ ë°ì´í„°
# DataFrame.duplicated():ì¤‘ë³µëœ ë°ì´í„° ì—¬ë¶€ë¡œ ë¶ˆë¦¬ëŠ” ê°’ ë°˜í™˜
# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html
# 
# DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False)[source]
# 
# ì´ìƒì¹˜(Outlier)
# ê°’ì˜ ë²”ìœ„ì—ì„œ ë²—ì–´ë‚˜ ê·¹ë‹¨ì ìœ¼ë¡œ í¬ê±°ë‚˜ ì‘ì€ ê°’.
# [ì´ìƒì¹˜ ì°¾ëŠ” ë²•]
# i)score:í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì´ìš©.
# í‰ê· ì„ ë¹¼ì£¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ  z score{ (Xâˆ’Î¼)/Ïƒ}ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
# zscoreë¥¼ ë„˜ëŠ” ë°ì´í„°ëŠ” ì´ìƒì¹˜.
# ì´ìƒì¹˜ ê¸°ì¤€ ì‘ìŒ-->íŒë‹¨ ë°ì´í„° ì¦ê°€
# ì´ìƒì¹˜ ê¸°ì¤€ ì¦ê°€-->íŒë‹¨ ë°ì´í„° ê°ì†Œ
# 
# [ì´ìƒì¹˜ íŒë‹¨ë²•]
# 
# 1. ì´ìƒì¹˜ ì‚­ì œ-->ë”°ë¡œ ë¶„ì„
# 2. ì´ìƒì¹˜ ëŒ€ì²´(ìµœëŒ“ê°’,ìµœì†Ÿê°’ ì„¤ì •)
# 3. ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡ê°’ í™œìš©
# 4. binning-->ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë°”ê¾¼ë‹¤.
# 
# http://colingorrie.github.io/outlier-detection.html
# 
# [z-score method]
#    <ì¸í’‹>
# -ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ë¦¬í„´ í•¨ìˆ˜:outlier
# -ë°ì´í„°í”„ë ˆì„:df
# -ì»¬ëŸ¼:col
# -ê¸°ì¤€:z
# 
# abs(df[col] - np.mean(df[col])) : |ë°ì´í„°- í‰ê· | 
# 
# abs(df[col] - np.mean(df[col]))/np.std(df[col]) : ì‘ì—…/ í‘œì¤€í¸ì°¨ 
# 
# df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index: ê°’>z ì¶”ì¶œ
# 
# [IQR method(ì‚¬ë¶„ìœ„ë²”ìœ„ìˆ˜)]
# 
# IQR=Q3 - Q1
# ì¦‰, IQRì€ ì œ3ì‚¬ë¶„ìœ„ìˆ˜~ ì œ1ì‚¬ë¶„ìœ„ ê°’ì„ ëº€ ê°’ìœ¼ë¡œ ì¤‘ê°„50%ë²”ìœ„
# ì´ìƒì¹˜  ê¸°ì¤€!
#  I) Q1 - 1.5 * IRQë³´ë‹¤ ì™¼ìª½
#  II) Q3 + 1.5 * IRQë³´ë‹¤ ì˜¤ë¥¸ìª½
#  
#  https://d3s0tskafalll9.cloudfront.net/media/images/F-19-1.max-800x600.jpg
#  
# ì •ê·œí™”(Normalization)
# https://realblack0.github.io/2020/03/29/normalization-standardization-regularization.html
# https://www.youtube.com/watch?v=FDCfw-YqWTE
# 
# * train ë°ì´í„°ì™€ test ë°ì´í„°ê°€ ë‚˜ëˆ ì ¸ ìˆëŠ” ê²½ìš° train ë°ì´í„°ë¥¼ ì •ê·œí™”ì‹œì¼°ë˜ ê¸°ì¤€ ê·¸ëŒ€ë¡œ test ë°ì´í„°ë„ ì •ê·œí™” ì‹œì¼œì¤˜ì•¼ í•©ë‹ˆë‹¤. *
# 
# :ì»¬ëŸ¼ ê°„ì— ë²”ìœ„ê°€ í¬ê²Œ ë‹¤ë¥¼ ê²½ìš° ë°ì´í„° ì •ê·œí™”.
# [ì •ê·œí™” í•˜ëŠ” ë°©ë²•]
# 1.í‘œì¤€í™”(Standardization)
#   * ë°ì´í„° í‰ê· ì€ 0, ë¶„ì‚°ì€ 1
#         
#         (Xâˆ’Î¼) / Ïƒ
# 2.Min-Max Scaling
#    * ë°ì´í„°ì˜ ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ë¡œ ë³€í™˜
#    
#    (Xâˆ’Xmin) / (Xmax -Xmin)
# 3,
# scikit-learnì˜ 
# StandardScaler, MinMaxScalerë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤.
#    
# ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)
# :ë¨¸ì‹  ëŸ¬ë‹ì´ë‚˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ ë²”ì£¼í˜• ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì´ìš©
# :ì´ì§„íŠ¹ì„±-1 / ë‚˜ë¨¸ì§€ -0
# 
# êµ¬ê°„í™”(Binning)
# :êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ê¸°
# 
# [ë°©ë²•]
# 1. Data binning
# 2. bucketing
#  +cut) êµ¬ê°„ ì •í•˜ê¸°
#  +value_counts():êµ¬ê°„ë³„ë¡œ ê°’ì´ ëª‡ ê°œê°€ ì†í•´ìˆëŠ”ì§€ í™•ì¸
#  +bins:ì •ìˆ˜ë¥¼ ì…ë ¥í•˜ë©´ ë°ì´í„°ì˜ ìµœì†Ÿê°’ì—ì„œ ìµœëŒ“ê°’ì„ ê· ë“±í•˜ê²Œ binsê°¯ìˆ˜ë§Œí¼ ë‚˜ëˆ ì¤€ë‹¤
#  +qcut: ë°ì´í„° ë¶„í¬ë¥¼ ë¹„ìŠ·í•œ í¬ê¸°ì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.
#  

# In[1]:


get_ipython().system(' mkdir -p ~/aiffel/data_preprocess/')
get_ipython().system(' ln -s ~/data/ ~/aiffel/data_preprocess/')


# In[2]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print("ğŸ‘½ Hello.")


# In[3]:


import os

csv_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/trade.csv'
trade = pd.read_csv(csv_file_path) 
trade.head()


# In[4]:


print('ì „ì²´ ë°ì´í„° ê±´ìˆ˜:', len(trade))


# In[10]:


print('ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜')
len(trade) - trade.count()


# In[12]:


trade.isnull()


# In[13]:


trade.isnull()


# In[14]:


trade[trade.isnull().any(axis=1)]


# In[15]:


trade.dropna(how='all', subset=['ìˆ˜ì¶œê±´ìˆ˜', 'ìˆ˜ì¶œê¸ˆì•¡', 'ìˆ˜ì…ê±´ìˆ˜', 'ìˆ˜ì…ê¸ˆì•¡', 'ë¬´ì—­ìˆ˜ì§€'], inplace=True)
print("ğŸ‘½ It's okay, no biggie.")


# In[16]:


trade[trade.isnull().any(axis=1)]


# In[17]:


trade.loc[[188, 191, 194]]


# In[18]:


trade.loc[191, 'ìˆ˜ì¶œê¸ˆì•¡'] = (trade.loc[188, 'ìˆ˜ì¶œê¸ˆì•¡'] + trade.loc[194, 'ìˆ˜ì¶œê¸ˆì•¡'] )/2
trade.loc[[191]]


# In[26]:


trade.loc[191, 'ë¬´ì—­ìˆ˜ì§€'] = trade.loc[191, 'ìˆ˜ì¶œê¸ˆì•¡'] - trade.loc[191, 'ìˆ˜ì…ê¸ˆì•¡'] 
trade.loc[[191]]


# In[25]:


trade.duplicated()


# In[27]:


trade[trade.duplicated()]


# In[28]:


trade[(trade['ê¸°ê°„']=='2020ë…„ 03ì›”')&(trade['êµ­ê°€ëª…']=='ì¤‘êµ­')]


# In[29]:


trade.drop_duplicates(inplace=True)
print("ğŸ‘½ It's okay, no biggie.")


# In[30]:


df = pd.DataFrame({'id':['001', '002', '003', '004', '002'], 
                   'name':['Park Yun', 'Kim Sung', 'Park Jin', 'Lee Han', 'Kim Min']})
df


# In[31]:


df.drop_duplicates(subset=['id'], keep='last')


# In[32]:


def outlier(df, col, z):
    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index
print("ğŸ‘½ It's okay, no biggie.")


# In[33]:


trade.loc[outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 1.5)]


# In[35]:


trade.loc[outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 1.5)]


# In[34]:


trade.loc[outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 2)]


# In[36]:


trade.loc[outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 3)]


# In[37]:


def not_outlier(df, col, z):
    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col]) <= z].index
print("ğŸ‘½ It's okay, no biggie.")


# In[38]:


trade.loc[not_outlier(trade, 'ë¬´ì—­ìˆ˜ì§€', 1.5)]


# In[39]:


np.random.seed(2020)
data = np.random.randn(100)  # í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì˜ ë¶„í¬ì—ì„œ 100ê°œì˜ ìˆ«ìë¥¼ ìƒ˜í”Œë§í•œ ë°ì´í„° ìƒì„±
data = np.concatenate((data, np.array([8, 10, -3, -5])))      # [8, 10, -3, -5])ë¥¼ ë°ì´í„° ë’¤ì— ì¶”ê°€í•¨
data


# In[40]:


fig, ax = plt.subplots()
ax.boxplot(data)
plt.show()


# In[41]:


Q3, Q1 = np.percentile(data, [75 ,25])
IQR = Q3 - Q1
IQR


# In[42]:


dataC


# In[47]:


def outlier2(df, col):
    q1 = df[col].quantile(0.32)
    q3 = df[col].quantile(0.56)
    iqr = q3 - q1
    return df[(df[col] < q1-1.5*iqr)|(df[col] > q3+1.5*iqr)]

outlier2(trade, 'ë¬´ì—­ìˆ˜ì§€')


# In[50]:


# ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ ëœë¤í•˜ê²Œ ë°ì´í„° xë¥¼ ìƒì„±í•©ë‹ˆë‹¤. 
np.random.seed(2020)
x = pd.DataFrame({'A': np.random.randn(100)*4+4,
                 'B': np.random.randn(100)-1})
x


# In[51]:


# ë°ì´í„° xë¥¼ Standardization ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
x_standardization = (x - x.mean())/x.std()
x_standardization


# In[52]:


# ë°ì´í„° xë¥¼ min-max scaling ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
x_min_max = (x-x.min())/(x.max()-x.min())
x_min_max


# In[53]:


fig, axs = plt.subplots(1,2, figsize=(12, 4),
                        gridspec_kw={'width_ratios': [2, 1]})

axs[0].scatter(x['A'], x['B'])
axs[0].set_xlim(-5, 15)
axs[0].set_ylim(-5, 5)
axs[0].axvline(c='grey', lw=1)
axs[0].axhline(c='grey', lw=1)
axs[0].set_title('Original Data')

axs[1].scatter(x_standardization['A'], x_standardization['B'])
axs[1].set_xlim(-5, 5)
axs[1].set_ylim(-5, 5)
axs[1].axvline(c='grey', lw=1)
axs[1].axhline(c='grey', lw=1)
axs[1].set_title('Data after standardization')

plt.show()


# In[54]:


fig, axs = plt.subplots(1,2, figsize=(12, 4),
                        gridspec_kw={'width_ratios': [2, 1]})

axs[0].scatter(x['A'], x['B'])
axs[0].set_xlim(-5, 15)
axs[0].set_ylim(-5, 5)
axs[0].axvline(c='grey', lw=1)
axs[0].axhline(c='grey', lw=1)
axs[0].set_title('Original Data')

axs[1].scatter(x_min_max['A'], x_min_max['B'])
axs[1].set_xlim(-5, 5)
axs[1].set_ylim(-5, 5)
axs[1].axvline(c='grey', lw=1)
axs[1].axhline(c='grey', lw=1)
axs[1].set_title('Data after min-max scaling')

plt.show()


# In[55]:


# trade ë°ì´í„°ë¥¼ Standardization ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
cols = ['ìˆ˜ì¶œê±´ìˆ˜', 'ìˆ˜ì¶œê¸ˆì•¡', 'ìˆ˜ì…ê±´ìˆ˜', 'ìˆ˜ì…ê¸ˆì•¡', 'ë¬´ì—­ìˆ˜ì§€']
trade_Standardization= (trade[cols]-trade[cols].mean())/trade[cols].std()
trade_Standardization.head()


# In[56]:


trade_Standardization.describe()


# In[57]:


# trade ë°ì´í„°ë¥¼ min-max scaling ê¸°ë²•ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. 
trade[cols] = (trade[cols]-trade[cols].min())/(trade[cols].max()-trade[cols].min())
trade.head()


# In[58]:


trade.describe()


# In[59]:


train = pd.DataFrame([[10, -10], [30, 10], [50, 0]])
test = pd.DataFrame([[0, 1], [10, 10]])
print("ğŸ‘½ It's okay, no biggie.")


# In[60]:


train_min = train.min()
train_max = train.max()

train_min_max = (train - train_min)/(train_max - train_min)
test_min_max =  (test - train_min)/(train_max - train_min)    # testë¥¼ min-max scalingí•  ë•Œë„ train ì •ê·œí™” ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰
print("ğŸ’« It's okay, no biggie...")


# In[61]:


train_min_max


# In[62]:


test_min_max


# In[63]:


from sklearn.preprocessing import MinMaxScaler
train = [[10, -10], [30, 10], [50, 0]]
test = [[0, 1]]
scaler = MinMaxScaler()
print("ğŸ‘½ It's okay, no biggie.")


# In[64]:


scaler.fit_transform(train)


# In[65]:


scaler.transform(test)


# In[66]:


#trade ë°ì´í„°ì˜ êµ­ê°€ëª… ì»¬ëŸ¼ ì›ë³¸
print(trade['êµ­ê°€ëª…'].head())  

# get_dummiesë¥¼ í†µí•´ êµ­ê°€ëª… ì›-í•« ì¸ì½”ë”©
country = pd.get_dummies(trade['êµ­ê°€ëª…'])
country.head()


# In[67]:


trade = pd.concat([trade, country], axis=1)
trade.head()


# In[71]:


salary = pd.Series([4300, 8370, 1750, 3830, 1840, 4220, 3020, 2290, 4740, 4600, 
                    2860, 3400, 4800, 4470, 2440, 4530, 4850, 4850, 4760, 4500, 
                    4640, 3000, 1880, 4880, 2240, 4750, 2750, 2810, 3100, 4290, 
                    1540, 2870, 1780, 4670, 4150, 2010, 3580, 1610, 2930, 4300, 
                    2740, 1680, 3490, 4350, 1680, 6420, 8740, 8980, 9080, 3990, 
                    4960, 3700, 9600, 9330, 5600, 4100, 1770, 8280, 3120, 1950, 
                    4210, 2020, 3820, 3170, 6330, 2570, 6940, 8610, 5060, 6370,
                    9080, 3760, 8060, 2500, 4660, 1770, 9220, 3380, 2490, 3450, 
                    1960, 7210, 5810, 9450, 8910, 3470, 7350, 8410, 7520, 9610, 
                    5150, 2630, 5610, 2750, 7050, 3350, 9450, 7140, 4170, 3090])
print("ğŸ‘½ Almost there..")


# In[72]:


salary.hist()


# In[73]:


bins = [0, 2000, 4000, 6000, 8000, 10000]
print("ğŸ‘½ Almost there..")


# In[74]:


ctg = pd.cut(salary, bins=bins)
ctg


# In[75]:


print('salary[0]:', salary[0])
print('salary[0]ê°€ ì†í•œ ì¹´í…Œê³ ë¦¬:', ctg[0])


# In[76]:


ctg.value_counts().sort_index()


# In[77]:


ctg = pd.cut(salary, bins=6)
ctg


# In[78]:


ctg.value_counts().sort_index()


# In[79]:


ctg = pd.qcut(salary, q=5)
ctg


# In[80]:


print(ctg.value_counts().sort_index())
print(".\n.\nğŸ›¸ Well done!")


# In[ ]:




