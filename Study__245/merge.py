{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "merge.py",
      "provenance": [],
      "authorship_tag": "ABX9TyMdJQnm+IcCAJueWN4I1whR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qsdcfd/TIL/blob/TIL/Study__245/merge.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1xXdmEQTR-p"
      },
      "outputs": [],
      "source": [
        "#직접 오픈 소스 코드가 있는 깃허브를 찾아서 필사했습니다.\n",
        "\n",
        "\"\"\"\n",
        "SQL-style merge routines\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations # from __future__ import 가 표시 될 때마다 최신 또는 예정된 Python 릴리스의 기능이 이전 버전으로 백 포트되었음을 의미\n",
        "\n",
        "import copy #레퍼런스 대입 시의 문제를 해결하기 위해서 copy모듈 사용합니다\n",
        "import datetime #시간 모듈\n",
        "from functools import partial #하나 이상의 인수가 이미 채워진 함수의 새 버전 사용, 함수의 새 버전은 그 자체를 기술\n",
        "\"\"\"\n",
        "지수 함수를 만든다고 가정\n",
        "\n",
        "아래 코드가 좋아보이긴 하지만 숫자가 많아지면 가독성이 떨어지게 될 것입니다.\n",
        "\n",
        "def power(base, exponent):\n",
        "\treturn base ** exponent\n",
        "\n",
        "def square(base):\n",
        "\treturn power(base,2)\n",
        "\n",
        "def cub(base):\n",
        "\treturn power(base,3)\n",
        "\n",
        "from functools import partial\n",
        "square = partial(power, exponent = 2)\n",
        "cube = parial(power, exponent =3)\n",
        "\n",
        "def test_partials():\n",
        "\tassert square(2) == 4\n",
        "    assert cube(2) == 8\n",
        "\n",
        "#  부분함수 톺아보기\n",
        "\n",
        "def test_partial_docs():\n",
        "\tassert square.keywords == {'exponent':2}\n",
        "    assert square.func == power\n",
        "    \n",
        "    assert cube.keywords == {'exponent':3}\n",
        "    assert cube.func == power\n",
        "\"\"\"\n",
        "\n",
        "import hashlib # 값이 거짓이면 제한된 환경에서 안전하지 않고 차단된 해싱 알고리즘 사용을 허락\n",
        "import string #문자열에 값을 치환\n",
        "from typing import(\n",
        "\tTYPE_CHECKING,\n",
        "    Hashable,\n",
        "    cast\n",
        ") #형 힌트 지원\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings #경고 메세지 삭제\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from pandas._libs import (\n",
        "    Timedelta,\n",
        "    hashtable as libhashtable,\n",
        "    join as libjoin,\n",
        "    lib,\n",
        ")\n",
        "from pandas._typing import (\n",
        "    ArrayLike,\n",
        "    DtypeObj,\n",
        "    IndexLabel,\n",
        "    Suffixes,\n",
        "    npt,\n",
        ")\n",
        "\n",
        "from pandas.errors import MergeError\n",
        "from pandas.util._decorators import (\n",
        "    Appender,\n",
        "    Substitution,\n",
        ")\n"
      ],
      "metadata": {
        "id": "IEMw7CJXYD__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.util._exceptions import find_stack_level\n",
        "\n",
        "from pandas.core.dtypes.cast import find_common_type\n",
        "from pandas.core.dtypes.common import (\n",
        "    ensure_float64,\n",
        "    ensure_int64,\n",
        "    ensure_object,\n",
        "    is_array_like,\n",
        "    is_bool,\n",
        "    is_bool_dtype,\n",
        "    is_categorical_dtype,\n",
        "    is_datetime64tz_dtype,\n",
        "    is_dtype_equal,\n",
        "    is_extension_array_dtype,\n",
        "    is_float_dtype,\n",
        "    is_integer,\n",
        "    is_integer_dtype,\n",
        "    is_list_like,\n",
        "    is_number,\n",
        "    is_numeric_dtype,\n",
        "    is_object_dtype,\n",
        "    needs_i8_conversion,\n",
        ")"
      ],
      "metadata": {
        "id": "ZgD6mH2KVCqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.dtypes.missing import (\n",
        "    isna,\n",
        "    na_value_for_dtype,\n",
        ")\n",
        "\n",
        "from pandas import (\n",
        "    Categorical,\n",
        "    Index,\n",
        "    MultiIndex,\n",
        "    Series,\n",
        ")\n",
        "from pandas.core import groupby\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas.core.arrays import ExtensionArray\n",
        "from pandas.core.arrays._mixins import NDArrayBackedExtensionArray\n",
        "import pandas.core.common as com\n",
        "from pandas.core.construction import extract_array\n",
        "from pandas.core.frame import _merge_doc\n",
        "from pandas.core.internals import concatenate_managers\n",
        "from pandas.core.sorting import is_int64_overflow_possible\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from pandas import DataFrame\n",
        "    from pandas.core.arrays import DatetimeArray\n"
      ],
      "metadata": {
        "id": "4EbF1ZWnYWug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Substitution(\"\\nleft : DataFrame or named Series\")\n",
        "@Appender(_merge_doc, indents=0)\n",
        "def merge(\n",
        "    left: DataFrame | Series,\n",
        "    right: DataFrame | Series,\n",
        "    how: str = \"inner\",\n",
        "    on: IndexLabel | None = None,\n",
        "    left_on: IndexLabel | None = None,\n",
        "    right_on: IndexLabel | None = None,\n",
        "    left_index: bool = False,\n",
        "    right_index: bool = False,\n",
        "    sort: bool = False,\n",
        "    suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "    copy: bool = True,\n",
        "    indicator: bool = False,\n",
        "    validate: str | None = None,\n",
        ") -> DataFrame:\n",
        "    op = _MergeOperation(\n",
        "        left,\n",
        "        right,\n",
        "        how=how,\n",
        "        on=on,\n",
        "        left_on=left_on,\n",
        "        right_on=right_on,\n",
        "        left_index=left_index,\n",
        "        right_index=right_index,\n",
        "        sort=sort,\n",
        "        suffixes=suffixes,\n",
        "        copy=copy,\n",
        "        indicator=indicator,\n",
        "        validate=validate,\n",
        "    )\n",
        "    return op.get_result()\n"
      ],
      "metadata": {
        "id": "Fpx6b8CfYWwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __debug__:\n",
        "    merge.__doc__ = _merge_doc % \"\\nleft : DataFrame\"\n",
        "\n",
        "\n",
        "def _groupby_and_merge(by, left: DataFrame, right: DataFrame, merge_pieces):\n",
        "    \"\"\"\n",
        "    groupby & merge; we are always performing a left-by type operation\n",
        "    Parameters\n",
        "    ----------\n",
        "    by: field to group\n",
        "    left: DataFrame\n",
        "    right: DataFrame\n",
        "    merge_pieces: function for merging\n",
        "    \"\"\"\n",
        "    pieces = []\n",
        "    if not isinstance(by, (list, tuple)):\n",
        "        by = [by]\n",
        "\n",
        "    lby = left.groupby(by, sort=False)\n",
        "    rby: groupby.DataFrameGroupBy | None = None\n",
        "\n",
        "    # if we can groupby the rhs\n",
        "    # then we can get vastly better perf\n",
        "    if all(item in right.columns for item in by):\n",
        "        rby = right.groupby(by, sort=False)\n",
        "\n",
        "    for key, lhs in lby:\n",
        "\n",
        "        if rby is None:\n",
        "            rhs = right\n",
        "        else:\n",
        "            try:\n",
        "                rhs = right.take(rby.indices[key])\n",
        "            except KeyError:\n",
        "                # key doesn't exist in left\n",
        "                lcols = lhs.columns.tolist()\n",
        "                cols = lcols + [r for r in right.columns if r not in set(lcols)]\n",
        "                merged = lhs.reindex(columns=cols)\n",
        "                merged.index = range(len(merged))\n",
        "                pieces.append(merged)\n",
        "                continue\n",
        "\n",
        "        merged = merge_pieces(lhs, rhs)\n",
        "\n",
        "        # make sure join keys are in the merged\n",
        "        # TODO, should merge_pieces do this?\n",
        "        merged[by] = key\n",
        "\n",
        "        pieces.append(merged)\n",
        "\n",
        "    # preserve the original order\n",
        "    # if we have a missing piece this can be reset\n",
        "    from pandas.core.reshape.concat import concat\n",
        "\n",
        "    result = concat(pieces, ignore_index=True)\n",
        "    result = result.reindex(columns=pieces[0].columns, copy=False)\n",
        "    return result, lby"
      ],
      "metadata": {
        "id": "Vpvzhp89YWzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_ordered(\n",
        "    left: DataFrame,\n",
        "    right: DataFrame,\n",
        "    on: IndexLabel | None = None,\n",
        "    left_on: IndexLabel | None = None,\n",
        "    right_on: IndexLabel | None = None,\n",
        "    left_by=None,\n",
        "    right_by=None,\n",
        "    fill_method: str | None = None,\n",
        "    suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "    how: str = \"outer\",\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Perform a merge for ordered data with optional filling/interpolation.\n",
        "    Designed for ordered data like time series data. Optionally\n",
        "    perform group-wise merge (see examples).\n",
        "    Parameters\n",
        "    ----------\n",
        "    left : DataFrame\n",
        "    right : DataFrame\n",
        "    on : label or list\n",
        "        Field names to join on. Must be found in both DataFrames.\n",
        "    left_on : label or list, or array-like\n",
        "        Field names to join on in left DataFrame. Can be a vector or list of\n",
        "        vectors of the length of the DataFrame to use a particular vector as\n",
        "        the join key instead of columns.\n",
        "    right_on : label or list, or array-like\n",
        "        Field names to join on in right DataFrame or vector/list of vectors per\n",
        "        left_on docs.\n",
        "    left_by : column name or list of column names\n",
        "        Group left DataFrame by group columns and merge piece by piece with\n",
        "        right DataFrame.\n",
        "    right_by : column name or list of column names\n",
        "        Group right DataFrame by group columns and merge piece by piece with\n",
        "        left DataFrame.\n",
        "    fill_method : {'ffill', None}, default None\n",
        "        Interpolation method for data.\n",
        "    suffixes : list-like, default is (\"_x\", \"_y\")\n",
        "        A length-2 sequence where each element is optionally a string\n",
        "        indicating the suffix to add to overlapping column names in\n",
        "        `left` and `right` respectively. Pass a value of `None` instead\n",
        "        of a string to indicate that the column name from `left` or\n",
        "        `right` should be left as-is, with no suffix. At least one of the\n",
        "        values must not be None.\n",
        "        .. versionchanged:: 0.25.0\n",
        "    how : {'left', 'right', 'outer', 'inner'}, default 'outer'\n",
        "        * left: use only keys from left frame (SQL: left outer join)\n",
        "        * right: use only keys from right frame (SQL: right outer join)\n",
        "        * outer: use union of keys from both frames (SQL: full outer join)\n",
        "        * inner: use intersection of keys from both frames (SQL: inner join).\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        The merged DataFrame output type will the be same as\n",
        "        'left', if it is a subclass of DataFrame.\n",
        "    See Also\n",
        "    --------\n",
        "    merge : Merge with a database-style join.\n",
        "    merge_asof : Merge on nearest keys.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> df1 = pd.DataFrame(\n",
        "    ...     {\n",
        "    ...         \"key\": [\"a\", \"c\", \"e\", \"a\", \"c\", \"e\"],\n",
        "    ...         \"lvalue\": [1, 2, 3, 1, 2, 3],\n",
        "    ...         \"group\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\n",
        "    ...     }\n",
        "    ... )\n",
        "    >>> df1\n",
        "          key  lvalue group\n",
        "    0   a       1     a\n",
        "    1   c       2     a\n",
        "    2   e       3     a\n",
        "    3   a       1     b\n",
        "    4   c       2     b\n",
        "    5   e       3     b\n",
        "    >>> df2 = pd.DataFrame({\"key\": [\"b\", \"c\", \"d\"], \"rvalue\": [1, 2, 3]})\n",
        "    >>> df2\n",
        "          key  rvalue\n",
        "    0   b       1\n",
        "    1   c       2\n",
        "    2   d       3\n",
        "    >>> merge_ordered(df1, df2, fill_method=\"ffill\", left_by=\"group\")\n",
        "      key  lvalue group  rvalue\n",
        "    0   a       1     a     NaN\n",
        "    1   b       1     a     1.0\n",
        "    2   c       2     a     2.0\n",
        "    3   d       2     a     3.0\n",
        "    4   e       3     a     3.0\n",
        "    5   a       1     b     NaN\n",
        "    6   b       1     b     1.0\n",
        "    7   c       2     b     2.0\n",
        "    8   d       2     b     3.0\n",
        "    9   e       3     b     3.0\n",
        "    \"\"\"\n",
        "\n",
        "    def _merger(x, y) -> DataFrame:\n",
        "        # perform the ordered merge operation\n",
        "        op = _OrderedMerge(\n",
        "            x,\n",
        "            y,\n",
        "            on=on,\n",
        "            left_on=left_on,\n",
        "            right_on=right_on,\n",
        "            suffixes=suffixes,\n",
        "            fill_method=fill_method,\n",
        "            how=how,\n",
        "        )\n",
        "        return op.get_result()\n",
        "\n",
        "    if left_by is not None and right_by is not None:\n",
        "        raise ValueError(\"Can only group either left or right frames\")\n",
        "    elif left_by is not None:\n",
        "        if isinstance(left_by, str):\n",
        "            left_by = [left_by]\n",
        "        check = set(left_by).difference(left.columns)\n",
        "        if len(check) != 0:\n",
        "            raise KeyError(f\"{check} not found in left columns\")\n",
        "        result, _ = _groupby_and_merge(left_by, left, right, lambda x, y: _merger(x, y))\n",
        "    elif right_by is not None:\n",
        "        if isinstance(right_by, str):\n",
        "            right_by = [right_by]\n",
        "        check = set(right_by).difference(right.columns)\n",
        "        if len(check) != 0:\n",
        "            raise KeyError(f\"{check} not found in right columns\")\n",
        "        result, _ = _groupby_and_merge(\n",
        "            right_by, right, left, lambda x, y: _merger(y, x)\n",
        "        )\n",
        "    else:\n",
        "        result = _merger(left, right)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "jvqB8gWqYW1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_asof(\n",
        "    left: DataFrame | Series,\n",
        "    right: DataFrame | Series,\n",
        "    on: IndexLabel | None = None,\n",
        "    left_on: IndexLabel | None = None,\n",
        "    right_on: IndexLabel | None = None,\n",
        "    left_index: bool = False,\n",
        "    right_index: bool = False,\n",
        "    by=None,\n",
        "    left_by=None,\n",
        "    right_by=None,\n",
        "    suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "    tolerance=None,\n",
        "    allow_exact_matches: bool = True,\n",
        "    direction: str = \"backward\",\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Perform a merge by key distance.\n",
        "    This is similar to a left-join except that we match on nearest\n",
        "    key rather than equal keys. Both DataFrames must be sorted by the key.\n",
        "    For each row in the left DataFrame:\n",
        "      - A \"backward\" search selects the last row in the right DataFrame whose\n",
        "        'on' key is less than or equal to the left's key.\n",
        "      - A \"forward\" search selects the first row in the right DataFrame whose\n",
        "        'on' key is greater than or equal to the left's key.\n",
        "      - A \"nearest\" search selects the row in the right DataFrame whose 'on'\n",
        "        key is closest in absolute distance to the left's key.\n",
        "    The default is \"backward\" and is compatible in versions below 0.20.0.\n",
        "    The direction parameter was added in version 0.20.0 and introduces\n",
        "    \"forward\" and \"nearest\".\n",
        "    Optionally match on equivalent keys with 'by' before searching with 'on'.\n",
        "    Parameters\n",
        "    ----------\n",
        "    left : DataFrame or named Series\n",
        "    right : DataFrame or named Series\n",
        "    on : label\n",
        "        Field name to join on. Must be found in both DataFrames.\n",
        "        The data MUST be ordered. Furthermore this must be a numeric column,\n",
        "        such as datetimelike, integer, or float. On or left_on/right_on\n",
        "        must be given.\n",
        "    left_on : label\n",
        "        Field name to join on in left DataFrame.\n",
        "    right_on : label\n",
        "        Field name to join on in right DataFrame.\n",
        "    left_index : bool\n",
        "        Use the index of the left DataFrame as the join key.\n",
        "    right_index : bool\n",
        "        Use the index of the right DataFrame as the join key.\n",
        "    by : column name or list of column names\n",
        "        Match on these columns before performing merge operation.\n",
        "    left_by : column name\n",
        "        Field names to match on in the left DataFrame.\n",
        "    right_by : column name\n",
        "        Field names to match on in the right DataFrame.\n",
        "    suffixes : 2-length sequence (tuple, list, ...)\n",
        "        Suffix to apply to overlapping column names in the left and right\n",
        "        side, respectively.\n",
        "    tolerance : int or Timedelta, optional, default None\n",
        "        Select asof tolerance within this range; must be compatible\n",
        "        with the merge index.\n",
        "    allow_exact_matches : bool, default True\n",
        "        - If True, allow matching with the same 'on' value\n",
        "          (i.e. less-than-or-equal-to / greater-than-or-equal-to)\n",
        "        - If False, don't match the same 'on' value\n",
        "          (i.e., strictly less-than / strictly greater-than).\n",
        "    direction : 'backward' (default), 'forward', or 'nearest'\n",
        "        Whether to search for prior, subsequent, or closest matches.\n",
        "    Returns\n",
        "    -------\n",
        "    merged : DataFrame\n",
        "    See Also\n",
        "    --------\n",
        "    merge : Merge with a database-style join.\n",
        "    merge_ordered : Merge with optional filling/interpolation.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> left = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\n",
        "    >>> left\n",
        "        a left_val\n",
        "    0   1        a\n",
        "    1   5        b\n",
        "    2  10        c\n",
        "    >>> right = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n",
        "    >>> right\n",
        "       a  right_val\n",
        "    0  1          1\n",
        "    1  2          2\n",
        "    2  3          3\n",
        "    3  6          6\n",
        "    4  7          7\n",
        "    >>> pd.merge_asof(left, right, on=\"a\")\n",
        "        a left_val  right_val\n",
        "    0   1        a          1\n",
        "    1   5        b          3\n",
        "    2  10        c          7\n",
        "    >>> pd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n",
        "        a left_val  right_val\n",
        "    0   1        a        NaN\n",
        "    1   5        b        3.0\n",
        "    2  10        c        7.0\n",
        "    >>> pd.merge_asof(left, right, on=\"a\", direction=\"forward\")\n",
        "        a left_val  right_val\n",
        "    0   1        a        1.0\n",
        "    1   5        b        6.0\n",
        "    2  10        c        NaN\n",
        "    >>> pd.merge_asof(left, right, on=\"a\", direction=\"nearest\")\n",
        "        a left_val  right_val\n",
        "    0   1        a          1\n",
        "    1   5        b          6\n",
        "    2  10        c          7\n",
        "    We can use indexed DataFrames as well.\n",
        "    >>> left = pd.DataFrame({\"left_val\": [\"a\", \"b\", \"c\"]}, index=[1, 5, 10])\n",
        "    >>> left\n",
        "       left_val\n",
        "    1         a\n",
        "    5         b\n",
        "    10        c\n",
        "    >>> right = pd.DataFrame({\"right_val\": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n",
        "    >>> right\n",
        "       right_val\n",
        "    1          1\n",
        "    2          2\n",
        "    3          3\n",
        "    6          6\n",
        "    7          7\n",
        "    >>> pd.merge_asof(left, right, left_index=True, right_index=True)\n",
        "       left_val  right_val\n",
        "    1         a          1\n",
        "    5         b          3\n",
        "    10        c          7\n",
        "    Here is a real-world times-series example\n",
        "    >>> quotes = pd.DataFrame(\n",
        "    ...     {\n",
        "    ...         \"time\": [\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.030\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.041\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.049\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.072\"),\n",
        "    ...             pd.Timestamp(\"2016-05-25 13:30:00.075\")\n",
        "    ...         ],\n",
        "    ...         \"ticker\": [\n",
        "    ...                \"GOOG\",\n",
        "    ...                \"MSFT\",\n",
        "    ...                \"MSFT\",\n",
        "    ...                \"MSFT\",\n",
        "    ...                \"GOOG\",\n",
        "    ...                \"AAPL\",\n",
        "    ...                \"GOOG\",\n",
        "    ...                \"MSFT\"\n",
        "    ...            ],\n",
        "    ...            \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n",
        "    ...            \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n",
        "    ...     }\n",
        "    ... )\n",
        "    >>> quotes\n",
        "                         time ticker     bid     ask\n",
        "    0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n",
        "    1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n",
        "    2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n",
        "    3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n",
        "    4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n",
        "    5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n",
        "    6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n",
        "    7 2016-05-25 13:30:00.075   MSFT   52.01   52.03\n",
        "    >>> trades = pd.DataFrame(\n",
        "    ...        {\n",
        "    ...            \"time\": [\n",
        "    ...                pd.Timestamp(\"2016-05-25 13:30:00.023\"),\n",
        "    ...                pd.Timestamp(\"2016-05-25 13:30:00.038\"),\n",
        "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
        "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\"),\n",
        "    ...                pd.Timestamp(\"2016-05-25 13:30:00.048\")\n",
        "    ...            ],\n",
        "    ...            \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n",
        "    ...            \"price\": [51.95, 51.95, 720.77, 720.92, 98.0],\n",
        "    ...            \"quantity\": [75, 155, 100, 100, 100]\n",
        "    ...        }\n",
        "    ...    )\n",
        "    >>> trades\n",
        "                         time ticker   price  quantity\n",
        "    0 2016-05-25 13:30:00.023   MSFT   51.95        75\n",
        "    1 2016-05-25 13:30:00.038   MSFT   51.95       155\n",
        "    2 2016-05-25 13:30:00.048   GOOG  720.77       100\n",
        "    3 2016-05-25 13:30:00.048   GOOG  720.92       100\n",
        "    4 2016-05-25 13:30:00.048   AAPL   98.00       100\n",
        "    By default we are taking the asof of the quotes\n",
        "    >>> pd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n",
        "                         time ticker   price  quantity     bid     ask\n",
        "    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n",
        "    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n",
        "    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n",
        "    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n",
        "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
        "    We only asof within 2ms between the quote time and the trade time\n",
        "    >>> pd.merge_asof(\n",
        "    ...     trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\")\n",
        "    ... )\n",
        "                         time ticker   price  quantity     bid     ask\n",
        "    0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n",
        "    1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n",
        "    2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n",
        "    3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n",
        "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
        "    We only asof within 10ms between the quote time and the trade time\n",
        "    and we exclude exact matches on time. However *prior* data will\n",
        "    propagate forward\n",
        "    >>> pd.merge_asof(\n",
        "    ...     trades,\n",
        "    ...     quotes,\n",
        "    ...     on=\"time\",\n",
        "    ...     by=\"ticker\",\n",
        "    ...     tolerance=pd.Timedelta(\"10ms\"),\n",
        "    ...     allow_exact_matches=False\n",
        "    ... )\n",
        "                         time ticker   price  quantity     bid     ask\n",
        "    0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n",
        "    1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n",
        "    2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n",
        "    3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n",
        "    4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\n",
        "    \"\"\"\n",
        "    op = _AsOfMerge(\n",
        "        left,\n",
        "        right,\n",
        "        on=on,\n",
        "        left_on=left_on,\n",
        "        right_on=right_on,\n",
        "        left_index=left_index,\n",
        "        right_index=right_index,\n",
        "        by=by,\n",
        "        left_by=left_by,\n",
        "        right_by=right_by,\n",
        "        suffixes=suffixes,\n",
        "        how=\"asof\",\n",
        "        tolerance=tolerance,\n",
        "        allow_exact_matches=allow_exact_matches,\n",
        "        direction=direction,\n",
        "    )\n",
        "    return op.get_result()"
      ],
      "metadata": {
        "id": "CHY5OnzaYW36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: transformations??\n",
        "# TODO: only copy DataFrames when modification necessary\n",
        "class _MergeOperation:\n",
        "    \"\"\"\n",
        "    Perform a database (SQL) merge operation between two DataFrame or Series\n",
        "    objects using either columns as keys or their row indexes\n",
        "    \"\"\"\n",
        "\n",
        "    _merge_type = \"merge\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        left: DataFrame | Series,\n",
        "        right: DataFrame | Series,\n",
        "        how: str = \"inner\",\n",
        "        on: IndexLabel | None = None,\n",
        "        left_on: IndexLabel | None = None,\n",
        "        right_on: IndexLabel | None = None,\n",
        "        axis: int = 1,\n",
        "        left_index: bool = False,\n",
        "        right_index: bool = False,\n",
        "        sort: bool = True,\n",
        "        suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "        copy: bool = True,\n",
        "        indicator: bool = False,\n",
        "        validate: str | None = None,\n",
        "    ) -> None:\n",
        "        _left = _validate_operand(left)\n",
        "        _right = _validate_operand(right)\n",
        "        self.left = self.orig_left = _left\n",
        "        self.right = self.orig_right = _right\n",
        "        self.how = how\n",
        "\n",
        "        # bm_axis -> the axis on the BlockManager\n",
        "        self.bm_axis = axis\n",
        "        # axis --> the axis on the Series/DataFrame\n",
        "        self.axis = 1 - axis if self.left.ndim == 2 else 0\n",
        "\n",
        "        self.on = com.maybe_make_list(on)\n",
        "        self.left_on = com.maybe_make_list(left_on)\n",
        "        self.right_on = com.maybe_make_list(right_on)\n",
        "\n",
        "        self.copy = copy\n",
        "        self.suffixes = suffixes\n",
        "        self.sort = sort\n",
        "\n",
        "        self.left_index = left_index\n",
        "        self.right_index = right_index\n",
        "\n",
        "        self.indicator = indicator\n",
        "\n",
        "        self.indicator_name: str | None\n",
        "        if isinstance(self.indicator, str):\n",
        "            self.indicator_name = self.indicator\n",
        "        elif isinstance(self.indicator, bool):\n",
        "            self.indicator_name = \"_merge\" if self.indicator else None\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"indicator option can only accept boolean or string arguments\"\n",
        "            )\n",
        "\n",
        "        if not is_bool(left_index):\n",
        "            raise ValueError(\n",
        "                f\"left_index parameter must be of type bool, not {type(left_index)}\"\n",
        "            )\n",
        "        if not is_bool(right_index):\n",
        "            raise ValueError(\n",
        "                f\"right_index parameter must be of type bool, not {type(right_index)}\"\n",
        "            )\n",
        "\n",
        "        # warn user when merging between different levels\n",
        "        if _left.columns.nlevels != _right.columns.nlevels:\n",
        "            msg = (\n",
        "                \"merging between different levels is deprecated and will be removed \"\n",
        "                f\"in a future version. ({left.columns.nlevels} levels on the left, \"\n",
        "                f\"{right.columns.nlevels} on the right)\"\n",
        "            )\n",
        "            # stacklevel chosen to be correct when this is reached via pd.merge\n",
        "            # (and not DataFrame.join)\n",
        "            warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())\n",
        "\n",
        "        self._validate_specification()\n",
        "\n",
        "        cross_col = None\n",
        "        if self.how == \"cross\":\n",
        "            (\n",
        "                self.left,\n",
        "                self.right,\n",
        "                self.how,\n",
        "                cross_col,\n",
        "            ) = self._create_cross_configuration(self.left, self.right)\n",
        "            self.left_on = self.right_on = [cross_col]\n",
        "        self._cross = cross_col\n",
        "\n",
        "        # note this function has side effects\n",
        "        (\n",
        "            self.left_join_keys,\n",
        "            self.right_join_keys,\n",
        "            self.join_names,\n",
        "        ) = self._get_merge_keys()\n",
        "\n",
        "        # validate the merge keys dtypes. We may need to coerce\n",
        "        # to avoid incompatible dtypes\n",
        "        self._maybe_coerce_merge_keys()\n",
        "\n",
        "        # If argument passed to validate,\n",
        "        # check if columns specified as unique\n",
        "        # are in fact unique.\n",
        "        if validate is not None:\n",
        "            self._validate(validate)\n",
        "    def get_result(self) -> DataFrame:\n",
        "        if self.indicator:\n",
        "            self.left, self.right = self._indicator_pre_merge(self.left, self.right)\n",
        "\n",
        "        join_index, left_indexer, right_indexer = self._get_join_info()\n",
        "\n",
        "        llabels, rlabels = _items_overlap_with_suffix(\n",
        "            self.left._info_axis, self.right._info_axis, self.suffixes\n",
        "        )\n",
        "\n",
        "        lindexers = {1: left_indexer} if left_indexer is not None else {}\n",
        "        rindexers = {1: right_indexer} if right_indexer is not None else {}\n",
        "\n",
        "        result_data = concatenate_managers(\n",
        "            [(self.left._mgr, lindexers), (self.right._mgr, rindexers)],\n",
        "            axes=[llabels.append(rlabels), join_index],\n",
        "            concat_axis=0,\n",
        "            copy=self.copy,\n",
        "        )\n",
        "\n",
        "        typ = self.left._constructor\n",
        "        result = typ(result_data).__finalize__(self, method=self._merge_type)\n",
        "\n",
        "        if self.indicator:\n",
        "            result = self._indicator_post_merge(result)\n",
        "\n",
        "        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n",
        "\n",
        "        self._maybe_restore_index_levels(result)\n",
        "\n",
        "        self._maybe_drop_cross_column(result, self._cross)\n",
        "\n",
        "        return result.__finalize__(self, method=\"merge\")\n",
        "\n",
        "    def _maybe_drop_cross_column(\n",
        "        self, result: DataFrame, cross_col: str | None\n",
        "    ) -> None:\n",
        "        if cross_col is not None:\n",
        "            del result[cross_col]\n",
        "\n",
        "    def _indicator_pre_merge(\n",
        "        self, left: DataFrame, right: DataFrame\n",
        "    ) -> tuple[DataFrame, DataFrame]:\n",
        "\n",
        "        columns = left.columns.union(right.columns)\n",
        "\n",
        "        for i in [\"_left_indicator\", \"_right_indicator\"]:\n",
        "            if i in columns:\n",
        "                raise ValueError(\n",
        "                    \"Cannot use `indicator=True` option when \"\n",
        "                    f\"data contains a column named {i}\"\n",
        "                )\n",
        "        if self.indicator_name in columns:\n",
        "            raise ValueError(\n",
        "                \"Cannot use name of an existing column for indicator column\"\n",
        "            )\n",
        "\n",
        "        left = left.copy()\n",
        "        right = right.copy()\n",
        "\n",
        "        left[\"_left_indicator\"] = 1\n",
        "        left[\"_left_indicator\"] = left[\"_left_indicator\"].astype(\"int8\")\n",
        "\n",
        "        right[\"_right_indicator\"] = 2\n",
        "        right[\"_right_indicator\"] = right[\"_right_indicator\"].astype(\"int8\")\n",
        "\n",
        "        return left, right\n",
        "\n",
        "    def _indicator_post_merge(self, result: DataFrame) -> DataFrame:\n",
        "\n",
        "        result[\"_left_indicator\"] = result[\"_left_indicator\"].fillna(0)\n",
        "        result[\"_right_indicator\"] = result[\"_right_indicator\"].fillna(0)\n",
        "\n",
        "        result[self.indicator_name] = Categorical(\n",
        "            (result[\"_left_indicator\"] + result[\"_right_indicator\"]),\n",
        "            categories=[1, 2, 3],\n",
        "        )\n",
        "        result[self.indicator_name] = result[self.indicator_name].cat.rename_categories(\n",
        "            [\"left_only\", \"right_only\", \"both\"]\n",
        "        )\n",
        "\n",
        "        result = result.drop(labels=[\"_left_indicator\", \"_right_indicator\"], axis=1)\n",
        "        return result\n",
        "\n",
        "    def _maybe_restore_index_levels(self, result: DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Restore index levels specified as `on` parameters\n",
        "        Here we check for cases where `self.left_on` and `self.right_on` pairs\n",
        "        each reference an index level in their respective DataFrames. The\n",
        "        joined columns corresponding to these pairs are then restored to the\n",
        "        index of `result`.\n",
        "        **Note:** This method has side effects. It modifies `result` in-place\n",
        "        Parameters\n",
        "        ----------\n",
        "        result: DataFrame\n",
        "            merge result\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        names_to_restore = []\n",
        "        for name, left_key, right_key in zip(\n",
        "            self.join_names, self.left_on, self.right_on\n",
        "        ):\n",
        "            if (\n",
        "                self.orig_left._is_level_reference(left_key)\n",
        "                and self.orig_right._is_level_reference(right_key)\n",
        "                and left_key == right_key\n",
        "                and name not in result.index.names\n",
        "            ):\n",
        "\n",
        "                names_to_restore.append(name)\n",
        "\n",
        "        if names_to_restore:\n",
        "            result.set_index(names_to_restore, inplace=True)\n",
        "\n",
        "    def _maybe_add_join_keys(\n",
        "        self,\n",
        "        result: DataFrame,\n",
        "        left_indexer: np.ndarray | None,\n",
        "        right_indexer: np.ndarray | None,\n",
        "    ) -> None:\n",
        "\n",
        "        left_has_missing = None\n",
        "        right_has_missing = None\n",
        "\n",
        "        keys = zip(self.join_names, self.left_on, self.right_on)\n",
        "        for i, (name, lname, rname) in enumerate(keys):\n",
        "            if not _should_fill(lname, rname):\n",
        "                continue\n",
        "\n",
        "            take_left, take_right = None, None\n",
        "\n",
        "            if name in result:\n",
        "\n",
        "                if left_indexer is not None and right_indexer is not None:\n",
        "                    if name in self.left:\n",
        "\n",
        "                        if left_has_missing is None:\n",
        "                            left_has_missing = (left_indexer == -1).any()\n",
        "\n",
        "                        if left_has_missing:\n",
        "                            take_right = self.right_join_keys[i]\n",
        "\n",
        "                            if not is_dtype_equal(\n",
        "                                result[name].dtype, self.left[name].dtype\n",
        "                            ):\n",
        "                                take_left = self.left[name]._values\n",
        "\n",
        "                    elif name in self.right:\n",
        "\n",
        "                        if right_has_missing is None:\n",
        "                            right_has_missing = (right_indexer == -1).any()\n",
        "\n",
        "                        if right_has_missing:\n",
        "                            take_left = self.left_join_keys[i]\n",
        "\n",
        "                            if not is_dtype_equal(\n",
        "                                result[name].dtype, self.right[name].dtype\n",
        "                            ):\n",
        "                                take_right = self.right[name]._values\n",
        "\n",
        "            elif left_indexer is not None and is_array_like(self.left_join_keys[i]):\n",
        "                take_left = self.left_join_keys[i]\n",
        "                take_right = self.right_join_keys[i]\n",
        "\n",
        "            if take_left is not None or take_right is not None:\n",
        "\n",
        "                if take_left is None:\n",
        "                    lvals = result[name]._values\n",
        "                else:\n",
        "                    # TODO: can we pin down take_left's type earlier?\n",
        "                    take_left = extract_array(take_left, extract_numpy=True)\n",
        "                    lfill = na_value_for_dtype(take_left.dtype)\n",
        "                    lvals = algos.take_nd(take_left, left_indexer, fill_value=lfill)\n",
        "\n",
        "                if take_right is None:\n",
        "                    rvals = result[name]._values\n",
        "                else:\n",
        "                    # TODO: can we pin down take_right's type earlier?\n",
        "                    take_right = extract_array(take_right, extract_numpy=True)\n",
        "                    rfill = na_value_for_dtype(take_right.dtype)\n",
        "                    rvals = algos.take_nd(take_right, right_indexer, fill_value=rfill)\n",
        "\n",
        "                # if we have an all missing left_indexer\n",
        "                # make sure to just use the right values or vice-versa\n",
        "                mask_left = left_indexer == -1\n",
        "                mask_right = right_indexer == -1\n",
        "                # error: Item \"bool\" of \"Union[Any, bool]\" has no attribute \"all\"\n",
        "                if mask_left.all():  # type: ignore[union-attr]\n",
        "                    key_col = Index(rvals)\n",
        "                    result_dtype = rvals.dtype\n",
        "                # error: Item \"bool\" of \"Union[Any, bool]\" has no attribute \"all\"\n",
        "                elif (\n",
        "                    right_indexer is not None\n",
        "                    and mask_right.all()  # type: ignore[union-attr]\n",
        "                ):\n",
        "                    key_col = Index(lvals)\n",
        "                    result_dtype = lvals.dtype\n",
        "                else:\n",
        "                    key_col = Index(lvals).where(~mask_left, rvals)\n",
        "                    result_dtype = find_common_type([lvals.dtype, rvals.dtype])\n",
        "\n",
        "                if result._is_label_reference(name):\n",
        "                    result[name] = Series(\n",
        "                        key_col, dtype=result_dtype, index=result.index\n",
        "                    )\n",
        "                elif result._is_level_reference(name):\n",
        "                    if isinstance(result.index, MultiIndex):\n",
        "                        key_col.name = name\n",
        "                        idx_list = [\n",
        "                            result.index.get_level_values(level_name)\n",
        "                            if level_name != name\n",
        "                            else key_col\n",
        "                            for level_name in result.index.names\n",
        "                        ]\n",
        "\n",
        "                        result.set_index(idx_list, inplace=True)\n",
        "                    else:\n",
        "                        result.index = Index(key_col, name=name)\n",
        "                else:\n",
        "                    result.insert(i, name or f\"key_{i}\", key_col)\n",
        "\n",
        "    def _get_join_indexers(self) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "        \"\"\"return the join indexers\"\"\"\n",
        "        return get_join_indexers(\n",
        "            self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
        "        )\n",
        "\n",
        "    def _get_join_info(\n",
        "        self,\n",
        "    ) -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp] | None]:\n",
        "\n",
        "        left_ax = self.left.axes[self.axis]\n",
        "        right_ax = self.right.axes[self.axis]\n",
        "\n",
        "        if self.left_index and self.right_index and self.how != \"asof\":\n",
        "            join_index, left_indexer, right_indexer = left_ax.join(\n",
        "                right_ax, how=self.how, return_indexers=True, sort=self.sort\n",
        "            )\n",
        "\n",
        "        elif self.right_index and self.how == \"left\":\n",
        "            join_index, left_indexer, right_indexer = _left_join_on_index(\n",
        "                left_ax, right_ax, self.left_join_keys, sort=self.sort\n",
        "            )\n",
        "\n",
        "        elif self.left_index and self.how == \"right\":\n",
        "            join_index, right_indexer, left_indexer = _left_join_on_index(\n",
        "                right_ax, left_ax, self.right_join_keys, sort=self.sort\n",
        "            )\n",
        "        else:\n",
        "            (left_indexer, right_indexer) = self._get_join_indexers()\n",
        "\n",
        "            if self.right_index:\n",
        "                if len(self.left) > 0:\n",
        "                    join_index = self._create_join_index(\n",
        "                        self.left.index,\n",
        "                        self.right.index,\n",
        "                        left_indexer,\n",
        "                        how=\"right\",\n",
        "                    )\n",
        "                else:\n",
        "                    join_index = self.right.index.take(right_indexer)\n",
        "            elif self.left_index:\n",
        "                if self.how == \"asof\":\n",
        "                    # GH#33463 asof should always behave like a left merge\n",
        "                    join_index = self._create_join_index(\n",
        "                        self.left.index,\n",
        "                        self.right.index,\n",
        "                        left_indexer,\n",
        "                        how=\"left\",\n",
        "                    )\n",
        "\n",
        "                elif len(self.right) > 0:\n",
        "                    join_index = self._create_join_index(\n",
        "                        self.right.index,\n",
        "                        self.left.index,\n",
        "                        right_indexer,\n",
        "                        how=\"left\",\n",
        "                    )\n",
        "                else:\n",
        "                    join_index = self.left.index.take(left_indexer)\n",
        "            else:\n",
        "                join_index = Index(np.arange(len(left_indexer)))\n",
        "\n",
        "        if len(join_index) == 0:\n",
        "            join_index = join_index.astype(object)\n",
        "        return join_index, left_indexer, right_indexer\n",
        "\n",
        "    def _create_join_index(\n",
        "        self,\n",
        "        index: Index,\n",
        "        other_index: Index,\n",
        "        indexer: npt.NDArray[np.intp],\n",
        "        how: str = \"left\",\n",
        "    ) -> Index:\n",
        "        \"\"\"\n",
        "        Create a join index by rearranging one index to match another\n",
        "        Parameters\n",
        "        ----------\n",
        "        index : Index being rearranged\n",
        "        other_index : Index used to supply values not found in index\n",
        "        indexer : np.ndarray[np.intp] how to rearrange index\n",
        "        how : str\n",
        "            Replacement is only necessary if indexer based on other_index.\n",
        "        Returns\n",
        "        -------\n",
        "        Index\n",
        "        \"\"\"\n",
        "        if self.how in (how, \"outer\") and not isinstance(other_index, MultiIndex):\n",
        "            # if final index requires values in other_index but not target\n",
        "            # index, indexer may hold missing (-1) values, causing Index.take\n",
        "            # to take the final value in target index. So, we set the last\n",
        "            # element to be the desired fill value. We do not use allow_fill\n",
        "            # and fill_value because it throws a ValueError on integer indices\n",
        "            mask = indexer == -1\n",
        "            if np.any(mask):\n",
        "                fill_value = na_value_for_dtype(index.dtype, compat=False)\n",
        "                index = index.append(Index([fill_value]))\n",
        "        return index.take(indexer)\n",
        "\n",
        "    def _get_merge_keys(self):\n",
        "        \"\"\"\n",
        "        Note: has side effects (copy/delete key columns)\n",
        "        Parameters\n",
        "        ----------\n",
        "        left\n",
        "        right\n",
        "        on\n",
        "        Returns\n",
        "        -------\n",
        "        left_keys, right_keys\n",
        "        \"\"\"\n",
        "        left_keys = []\n",
        "        right_keys = []\n",
        "        # error: Need type annotation for 'join_names' (hint: \"join_names: List[<type>]\n",
        "        # = ...\")\n",
        "        join_names = []  # type: ignore[var-annotated]\n",
        "        right_drop = []\n",
        "        left_drop = []\n",
        "\n",
        "        left, right = self.left, self.right\n",
        "\n",
        "        is_lkey = lambda x: is_array_like(x) and len(x) == len(left)\n",
        "        is_rkey = lambda x: is_array_like(x) and len(x) == len(right)\n",
        "\n",
        "        # Note that pd.merge_asof() has separate 'on' and 'by' parameters. A\n",
        "        # user could, for example, request 'left_index' and 'left_by'. In a\n",
        "        # regular pd.merge(), users cannot specify both 'left_index' and\n",
        "        # 'left_on'. (Instead, users have a MultiIndex). That means the\n",
        "        # self.left_on in this function is always empty in a pd.merge(), but\n",
        "        # a pd.merge_asof(left_index=True, left_by=...) will result in a\n",
        "        # self.left_on array with a None in the middle of it. This requires\n",
        "        # a work-around as designated in the code below.\n",
        "        # See _validate_specification() for where this happens.\n",
        "\n",
        "        # ugh, spaghetti re #733\n",
        "        if _any(self.left_on) and _any(self.right_on):\n",
        "            for lk, rk in zip(self.left_on, self.right_on):\n",
        "                if is_lkey(lk):\n",
        "                    left_keys.append(lk)\n",
        "                    if is_rkey(rk):\n",
        "                        right_keys.append(rk)\n",
        "                        join_names.append(None)  # what to do?\n",
        "                    else:\n",
        "                        if rk is not None:\n",
        "                            right_keys.append(right._get_label_or_level_values(rk))\n",
        "                            join_names.append(rk)\n",
        "                        else:\n",
        "                            # work-around for merge_asof(right_index=True)\n",
        "                            right_keys.append(right.index)\n",
        "                            join_names.append(right.index.name)\n",
        "                else:\n",
        "                    if not is_rkey(rk):\n",
        "                        if rk is not None:\n",
        "                            right_keys.append(right._get_label_or_level_values(rk))\n",
        "                        else:\n",
        "                            # work-around for merge_asof(right_index=True)\n",
        "                            right_keys.append(right.index)\n",
        "                        if lk is not None and lk == rk:\n",
        "                            # avoid key upcast in corner case (length-0)\n",
        "                            if len(left) > 0:\n",
        "                                right_drop.append(rk)\n",
        "                            else:\n",
        "                                left_drop.append(lk)\n",
        "                    else:\n",
        "                        right_keys.append(rk)\n",
        "                    if lk is not None:\n",
        "                        left_keys.append(left._get_label_or_level_values(lk))\n",
        "                        join_names.append(lk)\n",
        "                    else:\n",
        "                        # work-around for merge_asof(left_index=True)\n",
        "                        left_keys.append(left.index)\n",
        "                        join_names.append(left.index.name)\n",
        "        elif _any(self.left_on):\n",
        "            for k in self.left_on:\n",
        "                if is_lkey(k):\n",
        "                    left_keys.append(k)\n",
        "                    join_names.append(None)\n",
        "                else:\n",
        "                    left_keys.append(left._get_label_or_level_values(k))\n",
        "                    join_names.append(k)\n",
        "            if isinstance(self.right.index, MultiIndex):\n",
        "                right_keys = [\n",
        "                    lev._values.take(lev_codes)\n",
        "                    for lev, lev_codes in zip(\n",
        "                        self.right.index.levels, self.right.index.codes\n",
        "                    )\n",
        "                ]\n",
        "            else:\n",
        "                right_keys = [self.right.index._values]\n",
        "        elif _any(self.right_on):\n",
        "            for k in self.right_on:\n",
        "                if is_rkey(k):\n",
        "                    right_keys.append(k)\n",
        "                    join_names.append(None)\n",
        "                else:\n",
        "                    right_keys.append(right._get_label_or_level_values(k))\n",
        "                    join_names.append(k)\n",
        "            if isinstance(self.left.index, MultiIndex):\n",
        "                left_keys = [\n",
        "                    lev._values.take(lev_codes)\n",
        "                    for lev, lev_codes in zip(\n",
        "                        self.left.index.levels, self.left.index.codes\n",
        "                    )\n",
        "                ]\n",
        "            else:\n",
        "                left_keys = [self.left.index._values]\n",
        "\n",
        "        if left_drop:\n",
        "            self.left = self.left._drop_labels_or_levels(left_drop)\n",
        "\n",
        "        if right_drop:\n",
        "            self.right = self.right._drop_labels_or_levels(right_drop)\n",
        "\n",
        "        return left_keys, right_keys, join_names\n",
        "\n",
        "    def _maybe_coerce_merge_keys(self) -> None:\n",
        "        # we have valid merges but we may have to further\n",
        "        # coerce these if they are originally incompatible types\n",
        "        #\n",
        "        # for example if these are categorical, but are not dtype_equal\n",
        "        # or if we have object and integer dtypes\n",
        "\n",
        "        for lk, rk, name in zip(\n",
        "            self.left_join_keys, self.right_join_keys, self.join_names\n",
        "        ):\n",
        "            if (len(lk) and not len(rk)) or (not len(lk) and len(rk)):\n",
        "                continue\n",
        "\n",
        "            lk_is_cat = is_categorical_dtype(lk.dtype)\n",
        "            rk_is_cat = is_categorical_dtype(rk.dtype)\n",
        "            lk_is_object = is_object_dtype(lk.dtype)\n",
        "            rk_is_object = is_object_dtype(rk.dtype)\n",
        "\n",
        "            # if either left or right is a categorical\n",
        "            # then the must match exactly in categories & ordered\n",
        "            if lk_is_cat and rk_is_cat:\n",
        "                if lk._categories_match_up_to_permutation(rk):\n",
        "                    continue\n",
        "\n",
        "            elif lk_is_cat or rk_is_cat:\n",
        "                pass\n",
        "\n",
        "            elif is_dtype_equal(lk.dtype, rk.dtype):\n",
        "                continue\n",
        "\n",
        "            msg = (\n",
        "                f\"You are trying to merge on {lk.dtype} and \"\n",
        "                f\"{rk.dtype} columns. If you wish to proceed you should use pd.concat\"\n",
        "            )\n",
        "\n",
        "            # if we are numeric, then allow differing\n",
        "            # kinds to proceed, eg. int64 and int8, int and float\n",
        "            # further if we are object, but we infer to\n",
        "            # the same, then proceed\n",
        "            if is_numeric_dtype(lk.dtype) and is_numeric_dtype(rk.dtype):\n",
        "                if lk.dtype.kind == rk.dtype.kind:\n",
        "                    continue\n",
        "\n",
        "                # check whether ints and floats\n",
        "                elif is_integer_dtype(rk.dtype) and is_float_dtype(lk.dtype):\n",
        "                    if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n",
        "                        warnings.warn(\n",
        "                            \"You are merging on int and float \"\n",
        "                            \"columns where the float values \"\n",
        "                            \"are not equal to their int representation.\",\n",
        "                            UserWarning,\n",
        "                        )\n",
        "                    continue\n",
        "\n",
        "                elif is_float_dtype(rk.dtype) and is_integer_dtype(lk.dtype):\n",
        "                    if not (rk == rk.astype(lk.dtype))[~np.isnan(rk)].all():\n",
        "                        warnings.warn(\n",
        "                            \"You are merging on int and float \"\n",
        "                            \"columns where the float values \"\n",
        "                            \"are not equal to their int representation.\",\n",
        "                            UserWarning,\n",
        "                        )\n",
        "                    continue\n",
        "\n",
        "                # let's infer and see if we are ok\n",
        "                elif lib.infer_dtype(lk, skipna=False) == lib.infer_dtype(\n",
        "                    rk, skipna=False\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "            # Check if we are trying to merge on obviously\n",
        "            # incompatible dtypes GH 9780, GH 15800\n",
        "\n",
        "            # bool values are coerced to object\n",
        "            elif (lk_is_object and is_bool_dtype(rk.dtype)) or (\n",
        "                is_bool_dtype(lk.dtype) and rk_is_object\n",
        "            ):\n",
        "                pass\n",
        "\n",
        "            # object values are allowed to be merged\n",
        "            elif (lk_is_object and is_numeric_dtype(rk.dtype)) or (\n",
        "                is_numeric_dtype(lk.dtype) and rk_is_object\n",
        "            ):\n",
        "                inferred_left = lib.infer_dtype(lk, skipna=False)\n",
        "                inferred_right = lib.infer_dtype(rk, skipna=False)\n",
        "                bool_types = [\"integer\", \"mixed-integer\", \"boolean\", \"empty\"]\n",
        "                string_types = [\"string\", \"unicode\", \"mixed\", \"bytes\", \"empty\"]\n",
        "\n",
        "                # inferred bool\n",
        "                if inferred_left in bool_types and inferred_right in bool_types:\n",
        "                    pass\n",
        "\n",
        "                # unless we are merging non-string-like with string-like\n",
        "                elif (\n",
        "                    inferred_left in string_types and inferred_right not in string_types\n",
        "                ) or (\n",
        "                    inferred_right in string_types and inferred_left not in string_types\n",
        "                ):\n",
        "                    raise ValueError(msg)\n",
        "\n",
        "            # datetimelikes must match exactly\n",
        "            elif needs_i8_conversion(lk.dtype) and not needs_i8_conversion(rk.dtype):\n",
        "                raise ValueError(msg)\n",
        "            elif not needs_i8_conversion(lk.dtype) and needs_i8_conversion(rk.dtype):\n",
        "                raise ValueError(msg)\n",
        "            elif is_datetime64tz_dtype(lk.dtype) and not is_datetime64tz_dtype(\n",
        "                rk.dtype\n",
        "            ):\n",
        "                raise ValueError(msg)\n",
        "            elif not is_datetime64tz_dtype(lk.dtype) and is_datetime64tz_dtype(\n",
        "                rk.dtype\n",
        "            ):\n",
        "                raise ValueError(msg)\n",
        "\n",
        "            elif lk_is_object and rk_is_object:\n",
        "                continue\n",
        "\n",
        "            # Houston, we have a problem!\n",
        "            # let's coerce to object if the dtypes aren't\n",
        "            # categorical, otherwise coerce to the category\n",
        "            # dtype. If we coerced categories to object,\n",
        "            # then we would lose type information on some\n",
        "            # columns, and end up trying to merge\n",
        "            # incompatible dtypes. See GH 16900.\n",
        "            if name in self.left.columns:\n",
        "                typ = lk.categories.dtype if lk_is_cat else object\n",
        "                self.left = self.left.copy()\n",
        "                self.left[name] = self.left[name].astype(typ)\n",
        "            if name in self.right.columns:\n",
        "                typ = rk.categories.dtype if rk_is_cat else object\n",
        "                self.right = self.right.copy()\n",
        "                self.right[name] = self.right[name].astype(typ)\n",
        "\n",
        "    def _create_cross_configuration(\n",
        "        self, left: DataFrame, right: DataFrame\n",
        "    ) -> tuple[DataFrame, DataFrame, str, str]:\n",
        "        \"\"\"\n",
        "        Creates the configuration to dispatch the cross operation to inner join,\n",
        "        e.g. adding a join column and resetting parameters. Join column is added\n",
        "        to a new object, no inplace modification\n",
        "        Parameters\n",
        "        ----------\n",
        "        left : DataFrame\n",
        "        right : DataFrame\n",
        "        Returns\n",
        "        -------\n",
        "            a tuple (left, right, how, cross_col) representing the adjusted\n",
        "            DataFrames with cross_col, the merge operation set to inner and the column\n",
        "            to join over.\n",
        "        \"\"\"\n",
        "        cross_col = f\"_cross_{hashlib.md5().hexdigest()}\"\n",
        "        how = \"inner\"\n",
        "        return (\n",
        "            left.assign(**{cross_col: 1}),\n",
        "            right.assign(**{cross_col: 1}),\n",
        "            how,\n",
        "            cross_col,\n",
        "        )\n",
        "\n",
        "    def _validate_specification(self) -> None:\n",
        "        if self.how == \"cross\":\n",
        "            if (\n",
        "                self.left_index\n",
        "                or self.right_index\n",
        "                or self.right_on is not None\n",
        "                or self.left_on is not None\n",
        "                or self.on is not None\n",
        "            ):\n",
        "                raise MergeError(\n",
        "                    \"Can not pass on, right_on, left_on or set right_index=True or \"\n",
        "                    \"left_index=True\"\n",
        "                )\n",
        "            return\n",
        "        # Hm, any way to make this logic less complicated??\n",
        "        elif self.on is None and self.left_on is None and self.right_on is None:\n",
        "\n",
        "            if self.left_index and self.right_index:\n",
        "                self.left_on, self.right_on = (), ()\n",
        "            elif self.left_index:\n",
        "                raise MergeError(\"Must pass right_on or right_index=True\")\n",
        "            elif self.right_index:\n",
        "                raise MergeError(\"Must pass left_on or left_index=True\")\n",
        "            else:\n",
        "                # use the common columns\n",
        "                left_cols = self.left.columns\n",
        "                right_cols = self.right.columns\n",
        "                common_cols = left_cols.intersection(right_cols)\n",
        "                if len(common_cols) == 0:\n",
        "                    raise MergeError(\n",
        "                        \"No common columns to perform merge on. \"\n",
        "                        f\"Merge options: left_on={self.left_on}, \"\n",
        "                        f\"right_on={self.right_on}, \"\n",
        "                        f\"left_index={self.left_index}, \"\n",
        "                        f\"right_index={self.right_index}\"\n",
        "                    )\n",
        "                if (\n",
        "                    not left_cols.join(common_cols, how=\"inner\").is_unique\n",
        "                    or not right_cols.join(common_cols, how=\"inner\").is_unique\n",
        "                ):\n",
        "                    raise MergeError(f\"Data columns not unique: {repr(common_cols)}\")\n",
        "                self.left_on = self.right_on = common_cols\n",
        "        elif self.on is not None:\n",
        "            if self.left_on is not None or self.right_on is not None:\n",
        "                raise MergeError(\n",
        "                    'Can only pass argument \"on\" OR \"left_on\" '\n",
        "                    'and \"right_on\", not a combination of both.'\n",
        "                )\n",
        "            if self.left_index or self.right_index:\n",
        "                raise MergeError(\n",
        "                    'Can only pass argument \"on\" OR \"left_index\" '\n",
        "                    'and \"right_index\", not a combination of both.'\n",
        "                )\n",
        "            self.left_on = self.right_on = self.on\n",
        "        elif self.left_on is not None:\n",
        "            if self.left_index:\n",
        "                raise MergeError(\n",
        "                    'Can only pass argument \"left_on\" OR \"left_index\" not both.'\n",
        "                )\n",
        "            if not self.right_index and self.right_on is None:\n",
        "                raise MergeError('Must pass \"right_on\" OR \"right_index\".')\n",
        "            n = len(self.left_on)\n",
        "            if self.right_index:\n",
        "                if len(self.left_on) != self.right.index.nlevels:\n",
        "                    raise ValueError(\n",
        "                        \"len(left_on) must equal the number \"\n",
        "                        'of levels in the index of \"right\"'\n",
        "                    )\n",
        "                self.right_on = [None] * n\n",
        "        elif self.right_on is not None:\n",
        "            if self.right_index:\n",
        "                raise MergeError(\n",
        "                    'Can only pass argument \"right_on\" OR \"right_index\" not both.'\n",
        "                )\n",
        "            if not self.left_index and self.left_on is None:\n",
        "                raise MergeError('Must pass \"left_on\" OR \"left_index\".')\n",
        "            n = len(self.right_on)\n",
        "            if self.left_index:\n",
        "                if len(self.right_on) != self.left.index.nlevels:\n",
        "                    raise ValueError(\n",
        "                        \"len(right_on) must equal the number \"\n",
        "                        'of levels in the index of \"left\"'\n",
        "                    )\n",
        "                self.left_on = [None] * n\n",
        "        if self.how != \"cross\" and len(self.right_on) != len(self.left_on):\n",
        "            raise ValueError(\"len(right_on) must equal len(left_on)\")\n",
        "\n",
        "    def _validate(self, validate: str) -> None:\n",
        "\n",
        "        # Check uniqueness of each\n",
        "        if self.left_index:\n",
        "            left_unique = self.orig_left.index.is_unique\n",
        "        else:\n",
        "            left_unique = MultiIndex.from_arrays(self.left_join_keys).is_unique\n",
        "\n",
        "        if self.right_index:\n",
        "            right_unique = self.orig_right.index.is_unique\n",
        "        else:\n",
        "            right_unique = MultiIndex.from_arrays(self.right_join_keys).is_unique\n",
        "\n",
        "        # Check data integrity\n",
        "        if validate in [\"one_to_one\", \"1:1\"]:\n",
        "            if not left_unique and not right_unique:\n",
        "                raise MergeError(\n",
        "                    \"Merge keys are not unique in either left \"\n",
        "                    \"or right dataset; not a one-to-one merge\"\n",
        "                )\n",
        "            elif not left_unique:\n",
        "                raise MergeError(\n",
        "                    \"Merge keys are not unique in left dataset; not a one-to-one merge\"\n",
        "                )\n",
        "            elif not right_unique:\n",
        "                raise MergeError(\n",
        "                    \"Merge keys are not unique in right dataset; not a one-to-one merge\"\n",
        "                )\n",
        "\n",
        "        elif validate in [\"one_to_many\", \"1:m\"]:\n",
        "            if not left_unique:\n",
        "                raise MergeError(\n",
        "                    \"Merge keys are not unique in left dataset; not a one-to-many merge\"\n",
        "                )\n",
        "\n",
        "        elif validate in [\"many_to_one\", \"m:1\"]:\n",
        "            if not right_unique:\n",
        "                raise MergeError(\n",
        "                    \"Merge keys are not unique in right dataset; \"\n",
        "                    \"not a many-to-one merge\"\n",
        "                )\n",
        "\n",
        "        elif validate in [\"many_to_many\", \"m:m\"]:\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Not a valid argument for validate\")\n"
      ],
      "metadata": {
        "id": "fEYJ5W5KYW6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_join_indexers(\n",
        "    left_keys, right_keys, sort: bool = False, how: str = \"inner\", **kwargs\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    left_keys : ndarray, Index, Series\n",
        "    right_keys : ndarray, Index, Series\n",
        "    sort : bool, default False\n",
        "    how : {'inner', 'outer', 'left', 'right'}, default 'inner'\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray[np.intp]\n",
        "        Indexer into the left_keys.\n",
        "    np.ndarray[np.intp]\n",
        "        Indexer into the right_keys.\n",
        "    \"\"\"\n",
        "    assert len(left_keys) == len(\n",
        "        right_keys\n",
        "    ), \"left_key and right_keys must be the same length\"\n",
        "\n",
        "    # fast-path for empty left/right\n",
        "    left_n = len(left_keys[0])\n",
        "    right_n = len(right_keys[0])\n",
        "    if left_n == 0:\n",
        "        if how in [\"left\", \"inner\", \"cross\"]:\n",
        "            return _get_empty_indexer()\n",
        "        elif not sort and how in [\"right\", \"outer\"]:\n",
        "            return _get_no_sort_one_missing_indexer(right_n, True)\n",
        "    elif right_n == 0:\n",
        "        if how in [\"right\", \"inner\", \"cross\"]:\n",
        "            return _get_empty_indexer()\n",
        "        elif not sort and how in [\"left\", \"outer\"]:\n",
        "            return _get_no_sort_one_missing_indexer(left_n, False)\n",
        "\n",
        "    # get left & right join labels and num. of levels at each location\n",
        "    mapped = (\n",
        "        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)\n",
        "        for n in range(len(left_keys))\n",
        "    )\n",
        "    zipped = zip(*mapped)\n",
        "    llab, rlab, shape = (list(x) for x in zipped)\n",
        "\n",
        "    # get flat i8 keys from label lists\n",
        "    lkey, rkey = _get_join_keys(llab, rlab, shape, sort)\n",
        "\n",
        "    # factorize keys to a dense i8 space\n",
        "    # `count` is the num. of unique keys\n",
        "    # set(lkey) | set(rkey) == range(count)\n",
        "\n",
        "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n",
        "    # preserve left frame order if how == 'left' and sort == False\n",
        "    kwargs = copy.copy(kwargs)\n",
        "    if how in (\"left\", \"right\"):\n",
        "        kwargs[\"sort\"] = sort\n",
        "    join_func = {\n",
        "        \"inner\": libjoin.inner_join,\n",
        "        \"left\": libjoin.left_outer_join,\n",
        "        \"right\": lambda x, y, count, **kwargs: libjoin.left_outer_join(\n",
        "            y, x, count, **kwargs\n",
        "        )[::-1],\n",
        "        \"outer\": libjoin.full_outer_join,\n",
        "    }[how]\n",
        "\n",
        "    # error: Cannot call function of unknown type\n",
        "    return join_func(lkey, rkey, count, **kwargs)  # type: ignore[operator]\n"
      ],
      "metadata": {
        "id": "AOgaVF2GYW8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def restore_dropped_levels_multijoin(\n",
        "    left: MultiIndex,\n",
        "    right: MultiIndex,\n",
        "    dropped_level_names,\n",
        "    join_index: Index,\n",
        "    lindexer: npt.NDArray[np.intp],\n",
        "    rindexer: npt.NDArray[np.intp],\n",
        ") -> tuple[list[Index], npt.NDArray[np.intp], list[Hashable]]:\n",
        "    \"\"\"\n",
        "    *this is an internal non-public method*\n",
        "    Returns the levels, labels and names of a multi-index to multi-index join.\n",
        "    Depending on the type of join, this method restores the appropriate\n",
        "    dropped levels of the joined multi-index.\n",
        "    The method relies on lidx, rindexer which hold the index positions of\n",
        "    left and right, where a join was feasible\n",
        "    Parameters\n",
        "    ----------\n",
        "    left : MultiIndex\n",
        "        left index\n",
        "    right : MultiIndex\n",
        "        right index\n",
        "    dropped_level_names : str array\n",
        "        list of non-common level names\n",
        "    join_index : Index\n",
        "        the index of the join between the\n",
        "        common levels of left and right\n",
        "    lindexer : np.ndarray[np.intp]\n",
        "        left indexer\n",
        "    rindexer : np.ndarray[np.intp]\n",
        "        right indexer\n",
        "    Returns\n",
        "    -------\n",
        "    levels : list of Index\n",
        "        levels of combined multiindexes\n",
        "    labels : np.ndarray[np.intp]\n",
        "        labels of combined multiindexes\n",
        "    names : List[Hashable]\n",
        "        names of combined multiindex levels\n",
        "    \"\"\"\n",
        "\n",
        "    def _convert_to_multiindex(index: Index) -> MultiIndex:\n",
        "        if isinstance(index, MultiIndex):\n",
        "            return index\n",
        "        else:\n",
        "            return MultiIndex.from_arrays([index._values], names=[index.name])\n",
        "\n",
        "    # For multi-multi joins with one overlapping level,\n",
        "    # the returned index if of type Index\n",
        "    # Assure that join_index is of type MultiIndex\n",
        "    # so that dropped levels can be appended\n",
        "    join_index = _convert_to_multiindex(join_index)\n",
        "\n",
        "    join_levels = join_index.levels\n",
        "    join_codes = join_index.codes\n",
        "    join_names = join_index.names\n",
        "\n",
        "    # lindexer and rindexer hold the indexes where the join occurred\n",
        "    # for left and right respectively. If left/right is None then\n",
        "    # the join occurred on all indices of left/right\n",
        "    if lindexer is None:\n",
        "        lindexer = range(left.size)\n",
        "\n",
        "    if rindexer is None:\n",
        "        rindexer = range(right.size)\n",
        "\n",
        "    # Iterate through the levels that must be restored\n",
        "    for dropped_level_name in dropped_level_names:\n",
        "        if dropped_level_name in left.names:\n",
        "            idx = left\n",
        "            indexer = lindexer\n",
        "        else:\n",
        "            idx = right\n",
        "            indexer = rindexer\n",
        "\n",
        "        # The index of the level name to be restored\n",
        "        name_idx = idx.names.index(dropped_level_name)\n",
        "\n",
        "        restore_levels = idx.levels[name_idx]\n",
        "        # Inject -1 in the codes list where a join was not possible\n",
        "        # IOW indexer[i]=-1\n",
        "        codes = idx.codes[name_idx]\n",
        "        restore_codes = algos.take_nd(codes, indexer, fill_value=-1)\n",
        "\n",
        "        join_levels = join_levels + [restore_levels]\n",
        "        join_codes = join_codes + [restore_codes]\n",
        "        join_names = join_names + [dropped_level_name]\n",
        "\n",
        "    return join_levels, join_codes, join_names"
      ],
      "metadata": {
        "id": "xDm-Ugt3YW-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class _OrderedMerge(_MergeOperation):\n",
        "    _merge_type = \"ordered_merge\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        left: DataFrame | Series,\n",
        "        right: DataFrame | Series,\n",
        "        on: IndexLabel | None = None,\n",
        "        left_on: IndexLabel | None = None,\n",
        "        right_on: IndexLabel | None = None,\n",
        "        left_index: bool = False,\n",
        "        right_index: bool = False,\n",
        "        axis: int = 1,\n",
        "        suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "        copy: bool = True,\n",
        "        fill_method: str | None = None,\n",
        "        how: str = \"outer\",\n",
        "    ) -> None:\n",
        "\n",
        "        self.fill_method = fill_method\n",
        "        _MergeOperation.__init__(\n",
        "            self,\n",
        "            left,\n",
        "            right,\n",
        "            on=on,\n",
        "            left_on=left_on,\n",
        "            left_index=left_index,\n",
        "            right_index=right_index,\n",
        "            right_on=right_on,\n",
        "            axis=axis,\n",
        "            how=how,\n",
        "            suffixes=suffixes,\n",
        "            sort=True,  # factorize sorts\n",
        "        )\n",
        "\n",
        "    def get_result(self) -> DataFrame:\n",
        "        join_index, left_indexer, right_indexer = self._get_join_info()\n",
        "\n",
        "        llabels, rlabels = _items_overlap_with_suffix(\n",
        "            self.left._info_axis, self.right._info_axis, self.suffixes\n",
        "        )\n",
        "\n",
        "        left_join_indexer: np.ndarray | None\n",
        "        right_join_indexer: np.ndarray | None\n",
        "\n",
        "        if self.fill_method == \"ffill\":\n",
        "            if left_indexer is None:\n",
        "                raise TypeError(\"left_indexer cannot be None\")\n",
        "            left_indexer, right_indexer = cast(np.ndarray, left_indexer), cast(\n",
        "                np.ndarray, right_indexer\n",
        "            )\n",
        "            left_join_indexer = libjoin.ffill_indexer(left_indexer)\n",
        "            right_join_indexer = libjoin.ffill_indexer(right_indexer)\n",
        "        else:\n",
        "            left_join_indexer = left_indexer\n",
        "            right_join_indexer = right_indexer\n",
        "\n",
        "        lindexers = {1: left_join_indexer} if left_join_indexer is not None else {}\n",
        "        rindexers = {1: right_join_indexer} if right_join_indexer is not None else {}\n",
        "\n",
        "        result_data = concatenate_managers(\n",
        "            [(self.left._mgr, lindexers), (self.right._mgr, rindexers)],\n",
        "            axes=[llabels.append(rlabels), join_index],\n",
        "            concat_axis=0,\n",
        "            copy=self.copy,\n",
        "        )\n",
        "\n",
        "        typ = self.left._constructor\n",
        "        result = typ(result_data)\n",
        "\n",
        "        self._maybe_add_join_keys(result, left_indexer, right_indexer)\n",
        "\n",
        "        return result\n",
        "\n"
      ],
      "metadata": {
        "id": "83pKzQ8KYXAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _asof_by_function(direction: str):\n",
        "    name = f\"asof_join_{direction}_on_X_by_Y\"\n",
        "    return getattr(libjoin, name, None)\n",
        "\n",
        "\n",
        "_type_casters = {\n",
        "    \"int64_t\": ensure_int64,\n",
        "    \"double\": ensure_float64,\n",
        "    \"object\": ensure_object,\n",
        "}\n",
        "\n",
        "\n",
        "def _get_cython_type_upcast(dtype: DtypeObj) -> str:\n",
        "    \"\"\"Upcast a dtype to 'int64_t', 'double', or 'object'\"\"\"\n",
        "    if is_integer_dtype(dtype):\n",
        "        return \"int64_t\"\n",
        "    elif is_float_dtype(dtype):\n",
        "        return \"double\"\n",
        "    else:\n",
        "        return \"object\"\n",
        "\n",
        "\n",
        "class _AsOfMerge(_OrderedMerge):\n",
        "    _merge_type = \"asof_merge\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        left: DataFrame | Series,\n",
        "        right: DataFrame | Series,\n",
        "        on: IndexLabel | None = None,\n",
        "        left_on: IndexLabel | None = None,\n",
        "        right_on: IndexLabel | None = None,\n",
        "        left_index: bool = False,\n",
        "        right_index: bool = False,\n",
        "        by=None,\n",
        "        left_by=None,\n",
        "        right_by=None,\n",
        "        axis: int = 1,\n",
        "        suffixes: Suffixes = (\"_x\", \"_y\"),\n",
        "        copy: bool = True,\n",
        "        fill_method: str | None = None,\n",
        "        how: str = \"asof\",\n",
        "        tolerance=None,\n",
        "        allow_exact_matches: bool = True,\n",
        "        direction: str = \"backward\",\n",
        "    ) -> None:\n",
        "\n",
        "        self.by = by\n",
        "        self.left_by = left_by\n",
        "        self.right_by = right_by\n",
        "        self.tolerance = tolerance\n",
        "        self.allow_exact_matches = allow_exact_matches\n",
        "        self.direction = direction\n",
        "\n",
        "        _OrderedMerge.__init__(\n",
        "            self,\n",
        "            left,\n",
        "            right,\n",
        "            on=on,\n",
        "            left_on=left_on,\n",
        "            right_on=right_on,\n",
        "            left_index=left_index,\n",
        "            right_index=right_index,\n",
        "            axis=axis,\n",
        "            how=how,\n",
        "            suffixes=suffixes,\n",
        "            fill_method=fill_method,\n",
        "        )\n",
        "\n",
        "    def _validate_specification(self) -> None:\n",
        "        super()._validate_specification()\n",
        "\n",
        "        # we only allow on to be a single item for on\n",
        "        if len(self.left_on) != 1 and not self.left_index:\n",
        "            raise MergeError(\"can only asof on a key for left\")\n",
        "\n",
        "        if len(self.right_on) != 1 and not self.right_index:\n",
        "            raise MergeError(\"can only asof on a key for right\")\n",
        "\n",
        "        if self.left_index and isinstance(self.left.index, MultiIndex):\n",
        "            raise MergeError(\"left can only have one index\")\n",
        "\n",
        "        if self.right_index and isinstance(self.right.index, MultiIndex):\n",
        "            raise MergeError(\"right can only have one index\")\n",
        "\n",
        "        # set 'by' columns\n",
        "        if self.by is not None:\n",
        "            if self.left_by is not None or self.right_by is not None:\n",
        "                raise MergeError(\"Can only pass by OR left_by and right_by\")\n",
        "            self.left_by = self.right_by = self.by\n",
        "        if self.left_by is None and self.right_by is not None:\n",
        "            raise MergeError(\"missing left_by\")\n",
        "        if self.left_by is not None and self.right_by is None:\n",
        "            raise MergeError(\"missing right_by\")\n",
        "\n",
        "        # GH#29130 Check that merge keys do not have dtype object\n",
        "        if not self.left_index:\n",
        "            left_on = self.left_on[0]\n",
        "            if is_array_like(left_on):\n",
        "                lo_dtype = left_on.dtype\n",
        "            else:\n",
        "                lo_dtype = (\n",
        "                    self.left[left_on].dtype\n",
        "                    if left_on in self.left.columns\n",
        "                    else self.left.index.get_level_values(left_on)\n",
        "                )\n",
        "        else:\n",
        "            lo_dtype = self.left.index.dtype\n",
        "\n",
        "        if not self.right_index:\n",
        "            right_on = self.right_on[0]\n",
        "            if is_array_like(right_on):\n",
        "                ro_dtype = right_on.dtype\n",
        "            else:\n",
        "                ro_dtype = (\n",
        "                    self.right[right_on].dtype\n",
        "                    if right_on in self.right.columns\n",
        "                    else self.right.index.get_level_values(right_on)\n",
        "                )\n",
        "        else:\n",
        "            ro_dtype = self.right.index.dtype\n",
        "\n",
        "        if is_object_dtype(lo_dtype) or is_object_dtype(ro_dtype):\n",
        "            raise MergeError(\n",
        "                f\"Incompatible merge dtype, {repr(ro_dtype)} and \"\n",
        "                f\"{repr(lo_dtype)}, both sides must have numeric dtype\"\n",
        "            )\n",
        "\n",
        "        # add 'by' to our key-list so we can have it in the\n",
        "        # output as a key\n",
        "        if self.left_by is not None:\n",
        "            if not is_list_like(self.left_by):\n",
        "                self.left_by = [self.left_by]\n",
        "            if not is_list_like(self.right_by):\n",
        "                self.right_by = [self.right_by]\n",
        "\n",
        "            if len(self.left_by) != len(self.right_by):\n",
        "                raise MergeError(\"left_by and right_by must be same length\")\n",
        "\n",
        "            self.left_on = self.left_by + list(self.left_on)\n",
        "            self.right_on = self.right_by + list(self.right_on)\n",
        "\n",
        "        # check 'direction' is valid\n",
        "        if self.direction not in [\"backward\", \"forward\", \"nearest\"]:\n",
        "            raise MergeError(f\"direction invalid: {self.direction}\")\n",
        "\n",
        "    def _get_merge_keys(self):\n",
        "\n",
        "        # note this function has side effects\n",
        "        (left_join_keys, right_join_keys, join_names) = super()._get_merge_keys()\n",
        "\n",
        "        # validate index types are the same\n",
        "        for i, (lk, rk) in enumerate(zip(left_join_keys, right_join_keys)):\n",
        "            if not is_dtype_equal(lk.dtype, rk.dtype):\n",
        "                if is_categorical_dtype(lk.dtype) and is_categorical_dtype(rk.dtype):\n",
        "                    # The generic error message is confusing for categoricals.\n",
        "                    #\n",
        "                    # In this function, the join keys include both the original\n",
        "                    # ones of the merge_asof() call, and also the keys passed\n",
        "                    # to its by= argument. Unordered but equal categories\n",
        "                    # are not supported for the former, but will fail\n",
        "                    # later with a ValueError, so we don't *need* to check\n",
        "                    # for them here.\n",
        "                    msg = (\n",
        "                        f\"incompatible merge keys [{i}] {repr(lk.dtype)} and \"\n",
        "                        f\"{repr(rk.dtype)}, both sides category, but not equal ones\"\n",
        "                    )\n",
        "                else:\n",
        "                    msg = (\n",
        "                        f\"incompatible merge keys [{i}] {repr(lk.dtype)} and \"\n",
        "                        f\"{repr(rk.dtype)}, must be the same type\"\n",
        "                    )\n",
        "                raise MergeError(msg)\n",
        "\n",
        "        # validate tolerance; datetime.timedelta or Timedelta if we have a DTI\n",
        "        if self.tolerance is not None:\n",
        "\n",
        "            if self.left_index:\n",
        "                lt = self.left.index\n",
        "            else:\n",
        "                lt = left_join_keys[-1]\n",
        "\n",
        "            msg = (\n",
        "                f\"incompatible tolerance {self.tolerance}, must be compat \"\n",
        "                f\"with type {repr(lt.dtype)}\"\n",
        "            )\n",
        "\n",
        "            if needs_i8_conversion(lt):\n",
        "                if not isinstance(self.tolerance, datetime.timedelta):\n",
        "                    raise MergeError(msg)\n",
        "                if self.tolerance < Timedelta(0):\n",
        "                    raise MergeError(\"tolerance must be positive\")\n",
        "\n",
        "            elif is_integer_dtype(lt):\n",
        "                if not is_integer(self.tolerance):\n",
        "                    raise MergeError(msg)\n",
        "                if self.tolerance < 0:\n",
        "                    raise MergeError(\"tolerance must be positive\")\n",
        "\n",
        "            elif is_float_dtype(lt):\n",
        "                if not is_number(self.tolerance):\n",
        "                    raise MergeError(msg)\n",
        "                if self.tolerance < 0:\n",
        "                    raise MergeError(\"tolerance must be positive\")\n",
        "\n",
        "            else:\n",
        "                raise MergeError(\"key must be integer, timestamp or float\")\n",
        "\n",
        "        # validate allow_exact_matches\n",
        "        if not is_bool(self.allow_exact_matches):\n",
        "            msg = (\n",
        "                \"allow_exact_matches must be boolean, \"\n",
        "                f\"passed {self.allow_exact_matches}\"\n",
        "            )\n",
        "            raise MergeError(msg)\n",
        "\n",
        "        return left_join_keys, right_join_keys, join_names\n",
        "\n",
        "    def _get_join_indexers(self) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "        \"\"\"return the join indexers\"\"\"\n",
        "\n",
        "        def flip(xs) -> np.ndarray:\n",
        "            \"\"\"unlike np.transpose, this returns an array of tuples\"\"\"\n",
        "\n",
        "            def injection(obj):\n",
        "                if not is_extension_array_dtype(obj):\n",
        "                    # ndarray\n",
        "                    return obj\n",
        "                obj = extract_array(obj)\n",
        "                if isinstance(obj, NDArrayBackedExtensionArray):\n",
        "                    # fastpath for e.g. dt64tz, categorical\n",
        "                    return obj._ndarray\n",
        "                # FIXME: returning obj._values_for_argsort() here doesn't\n",
        "                #  break in any existing test cases, but i (@jbrockmendel)\n",
        "                #  am pretty sure it should!\n",
        "                #  e.g.\n",
        "                #  arr = pd.array([0, pd.NA, 255], dtype=\"UInt8\")\n",
        "                #  will have values_for_argsort (before GH#45434)\n",
        "                #  np.array([0, 255, 255], dtype=np.uint8)\n",
        "                #  and the non-injectivity should make a difference somehow\n",
        "                #  shouldn't it?\n",
        "                return np.asarray(obj)\n",
        "\n",
        "            xs = [injection(x) for x in xs]\n",
        "            labels = list(string.ascii_lowercase[: len(xs)])\n",
        "            dtypes = [x.dtype for x in xs]\n",
        "            labeled_dtypes = list(zip(labels, dtypes))\n",
        "            return np.array(list(zip(*xs)), labeled_dtypes)\n",
        "\n",
        "        # values to compare\n",
        "        left_values = (\n",
        "            self.left.index._values if self.left_index else self.left_join_keys[-1]\n",
        "        )\n",
        "        right_values = (\n",
        "            self.right.index._values if self.right_index else self.right_join_keys[-1]\n",
        "        )\n",
        "        tolerance = self.tolerance\n",
        "\n",
        "        # we require sortedness and non-null values in the join keys\n",
        "        if not Index(left_values).is_monotonic_increasing:\n",
        "            side = \"left\"\n",
        "            if isna(left_values).any():\n",
        "                raise ValueError(f\"Merge keys contain null values on {side} side\")\n",
        "            else:\n",
        "                raise ValueError(f\"{side} keys must be sorted\")\n",
        "\n",
        "        if not Index(right_values).is_monotonic_increasing:\n",
        "            side = \"right\"\n",
        "            if isna(right_values).any():\n",
        "                raise ValueError(f\"Merge keys contain null values on {side} side\")\n",
        "            else:\n",
        "                raise ValueError(f\"{side} keys must be sorted\")\n",
        "\n",
        "        # initial type conversion as needed\n",
        "        if needs_i8_conversion(left_values):\n",
        "            left_values = left_values.view(\"i8\")\n",
        "            right_values = right_values.view(\"i8\")\n",
        "            if tolerance is not None:\n",
        "                tolerance = Timedelta(tolerance)\n",
        "                tolerance = tolerance.value\n",
        "\n",
        "        # a \"by\" parameter requires special handling\n",
        "        if self.left_by is not None:\n",
        "            # remove 'on' parameter from values if one existed\n",
        "            if self.left_index and self.right_index:\n",
        "                left_by_values = self.left_join_keys\n",
        "                right_by_values = self.right_join_keys\n",
        "            else:\n",
        "                left_by_values = self.left_join_keys[0:-1]\n",
        "                right_by_values = self.right_join_keys[0:-1]\n",
        "\n",
        "            # get tuple representation of values if more than one\n",
        "            if len(left_by_values) == 1:\n",
        "                left_by_values = left_by_values[0]\n",
        "                right_by_values = right_by_values[0]\n",
        "            else:\n",
        "                # We get here with non-ndarrays in test_merge_by_col_tz_aware\n",
        "                #  and test_merge_groupby_multiple_column_with_categorical_column\n",
        "                left_by_values = flip(left_by_values)\n",
        "                right_by_values = flip(right_by_values)\n",
        "\n",
        "            # upcast 'by' parameter because HashTable is limited\n",
        "            by_type = _get_cython_type_upcast(left_by_values.dtype)\n",
        "            by_type_caster = _type_casters[by_type]\n",
        "            # error: Cannot call function of unknown type\n",
        "            left_by_values = by_type_caster(left_by_values)  # type: ignore[operator]\n",
        "            # error: Cannot call function of unknown type\n",
        "            right_by_values = by_type_caster(right_by_values)  # type: ignore[operator]\n",
        "\n",
        "            # choose appropriate function by type\n",
        "            func = _asof_by_function(self.direction)\n",
        "            return func(\n",
        "                left_values,\n",
        "                right_values,\n",
        "                left_by_values,\n",
        "                right_by_values,\n",
        "                self.allow_exact_matches,\n",
        "                tolerance,\n",
        "            )\n",
        "        else:\n",
        "            # choose appropriate function by type\n",
        "            func = _asof_by_function(self.direction)\n",
        "            return func(\n",
        "                left_values,\n",
        "                right_values,\n",
        "                None,\n",
        "                None,\n",
        "                self.allow_exact_matches,\n",
        "                tolerance,\n",
        "                False,\n",
        "            )\n",
        "\n"
      ],
      "metadata": {
        "id": "EXpumqr0YXDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_multiindex_indexer(\n",
        "    join_keys, index: MultiIndex, sort: bool\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "\n",
        "    # left & right join labels and num. of levels at each location\n",
        "    mapped = (\n",
        "        _factorize_keys(index.levels[n], join_keys[n], sort=sort)\n",
        "        for n in range(index.nlevels)\n",
        "    )\n",
        "    zipped = zip(*mapped)\n",
        "    rcodes, lcodes, shape = (list(x) for x in zipped)\n",
        "    if sort:\n",
        "        rcodes = list(map(np.take, rcodes, index.codes))\n",
        "    else:\n",
        "        i8copy = lambda a: a.astype(\"i8\", subok=False, copy=True)\n",
        "        rcodes = list(map(i8copy, index.codes))\n",
        "\n",
        "    # fix right labels if there were any nulls\n",
        "    for i in range(len(join_keys)):\n",
        "        mask = index.codes[i] == -1\n",
        "        if mask.any():\n",
        "            # check if there already was any nulls at this location\n",
        "            # if there was, it is factorized to `shape[i] - 1`\n",
        "            a = join_keys[i][lcodes[i] == shape[i] - 1]\n",
        "            if a.size == 0 or not a[0] != a[0]:\n",
        "                shape[i] += 1\n",
        "\n",
        "            rcodes[i][mask] = shape[i] - 1\n",
        "\n",
        "    # get flat i8 join keys\n",
        "    lkey, rkey = _get_join_keys(lcodes, rcodes, shape, sort)\n",
        "\n",
        "    # factorize keys to a dense i8 space\n",
        "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n",
        "\n",
        "    return libjoin.left_outer_join(lkey, rkey, count, sort=sort)\n",
        "\n",
        "\n",
        "def _get_single_indexer(\n",
        "    join_key, index: Index, sort: bool = False\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "    left_key, right_key, count = _factorize_keys(join_key, index._values, sort=sort)\n",
        "\n",
        "    return libjoin.left_outer_join(left_key, right_key, count, sort=sort)\n",
        "\n",
        "\n",
        "def _get_empty_indexer() -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "    \"\"\"Return empty join indexers.\"\"\"\n",
        "    return (\n",
        "        np.array([], dtype=np.intp),\n",
        "        np.array([], dtype=np.intp),\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_no_sort_one_missing_indexer(\n",
        "    n: int, left_missing: bool\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "    \"\"\"\n",
        "    Return join indexers where all of one side is selected without sorting\n",
        "    and none of the other side is selected.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n : int\n",
        "        Length of indexers to create.\n",
        "    left_missing : bool\n",
        "        If True, the left indexer will contain only -1's.\n",
        "        If False, the right indexer will contain only -1's.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray[np.intp]\n",
        "        Left indexer\n",
        "    np.ndarray[np.intp]\n",
        "        Right indexer\n",
        "    \"\"\"\n",
        "    idx = np.arange(n, dtype=np.intp)\n",
        "    idx_missing = np.full(shape=n, fill_value=-1, dtype=np.intp)\n",
        "    if left_missing:\n",
        "        return idx_missing, idx\n",
        "    return idx, idx_missing\n",
        "\n",
        "\n",
        "def _left_join_on_index(\n",
        "    left_ax: Index, right_ax: Index, join_keys, sort: bool = False\n",
        ") -> tuple[Index, npt.NDArray[np.intp] | None, npt.NDArray[np.intp]]:\n",
        "    if len(join_keys) > 1:\n",
        "        if not (\n",
        "            isinstance(right_ax, MultiIndex) and len(join_keys) == right_ax.nlevels\n",
        "        ):\n",
        "            raise AssertionError(\n",
        "                \"If more than one join key is given then \"\n",
        "                \"'right_ax' must be a MultiIndex and the \"\n",
        "                \"number of join keys must be the number of levels in right_ax\"\n",
        "            )\n",
        "\n",
        "        left_indexer, right_indexer = _get_multiindex_indexer(\n",
        "            join_keys, right_ax, sort=sort\n",
        "        )\n",
        "    else:\n",
        "        jkey = join_keys[0]\n",
        "\n",
        "        left_indexer, right_indexer = _get_single_indexer(jkey, right_ax, sort=sort)\n",
        "\n",
        "    if sort or len(left_ax) != len(left_indexer):\n",
        "        # if asked to sort or there are 1-to-many matches\n",
        "        join_index = left_ax.take(left_indexer)\n",
        "        return join_index, left_indexer, right_indexer\n",
        "\n",
        "    # left frame preserves order & length of its index\n",
        "    return left_ax, None, right_indexer\n",
        "\n"
      ],
      "metadata": {
        "id": "KE4AWyLeYXFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _factorize_keys(\n",
        "    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = \"inner\"\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:\n",
        "    \"\"\"\n",
        "    Encode left and right keys as enumerated types.\n",
        "    This is used to get the join indexers to be used when merging DataFrames.\n",
        "    Parameters\n",
        "    ----------\n",
        "    lk : array-like\n",
        "        Left key.\n",
        "    rk : array-like\n",
        "        Right key.\n",
        "    sort : bool, defaults to True\n",
        "        If True, the encoding is done such that the unique elements in the\n",
        "        keys are sorted.\n",
        "    how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’\n",
        "        Type of merge.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray[np.intp]\n",
        "        Left (resp. right if called with `key='right'`) labels, as enumerated type.\n",
        "    np.ndarray[np.intp]\n",
        "        Right (resp. left if called with `key='right'`) labels, as enumerated type.\n",
        "    int\n",
        "        Number of unique elements in union of left and right labels.\n",
        "    See Also\n",
        "    --------\n",
        "    merge : Merge DataFrame or named Series objects\n",
        "        with a database-style join.\n",
        "    algorithms.factorize : Encode the object as an enumerated type\n",
        "        or categorical variable.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> lk = np.array([\"a\", \"c\", \"b\"])\n",
        "    >>> rk = np.array([\"a\", \"c\"])\n",
        "    Here, the unique values are `'a', 'b', 'c'`. With the default\n",
        "    `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:\n",
        "    >>> pd.core.reshape.merge._factorize_keys(lk, rk)\n",
        "    (array([0, 2, 1]), array([0, 2]), 3)\n",
        "    With the `sort=False`, the encoding will correspond to the order\n",
        "    in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:\n",
        "    >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)\n",
        "    (array([0, 1, 2]), array([0, 1]), 3)\n",
        "    \"\"\"\n",
        "    # Some pre-processing for non-ndarray lk / rk\n",
        "    lk = extract_array(lk, extract_numpy=True, extract_range=True)\n",
        "    rk = extract_array(rk, extract_numpy=True, extract_range=True)\n",
        "    # TODO: if either is a RangeIndex, we can likely factorize more efficiently?\n",
        "\n",
        "    if is_datetime64tz_dtype(lk.dtype) and is_datetime64tz_dtype(rk.dtype):\n",
        "        # Extract the ndarray (UTC-localized) values\n",
        "        # Note: we dont need the dtypes to match, as these can still be compared\n",
        "        lk = cast(\"DatetimeArray\", lk)._ndarray\n",
        "        rk = cast(\"DatetimeArray\", rk)._ndarray\n",
        "\n",
        "    elif (\n",
        "        is_categorical_dtype(lk.dtype)\n",
        "        and is_categorical_dtype(rk.dtype)\n",
        "        and is_dtype_equal(lk.dtype, rk.dtype)\n",
        "    ):\n",
        "        assert isinstance(lk, Categorical)\n",
        "        assert isinstance(rk, Categorical)\n",
        "        # Cast rk to encoding so we can compare codes with lk\n",
        "\n",
        "        rk = lk._encode_with_my_categories(rk)\n",
        "\n",
        "        lk = ensure_int64(lk.codes)\n",
        "        rk = ensure_int64(rk.codes)\n",
        "\n",
        "    elif isinstance(lk, ExtensionArray) and is_dtype_equal(lk.dtype, rk.dtype):\n",
        "        lk, _ = lk._values_for_factorize()\n",
        "\n",
        "        # error: Item \"ndarray\" of \"Union[Any, ndarray]\" has no attribute\n",
        "        # \"_values_for_factorize\"\n",
        "        rk, _ = rk._values_for_factorize()  # type: ignore[union-attr]\n",
        "\n",
        "    klass: type[libhashtable.Factorizer] | type[libhashtable.Int64Factorizer]\n",
        "    if is_integer_dtype(lk.dtype) and is_integer_dtype(rk.dtype):\n",
        "        # GH#23917 TODO: needs tests for case where lk is integer-dtype\n",
        "        #  and rk is datetime-dtype\n",
        "        klass = libhashtable.Int64Factorizer\n",
        "        lk = ensure_int64(np.asarray(lk))\n",
        "        rk = ensure_int64(np.asarray(rk))\n",
        "\n",
        "    elif needs_i8_conversion(lk.dtype) and is_dtype_equal(lk.dtype, rk.dtype):\n",
        "        # GH#23917 TODO: Needs tests for non-matching dtypes\n",
        "        klass = libhashtable.Int64Factorizer\n",
        "        lk = ensure_int64(np.asarray(lk, dtype=np.int64))\n",
        "        rk = ensure_int64(np.asarray(rk, dtype=np.int64))\n",
        "\n",
        "    else:\n",
        "        klass = libhashtable.ObjectFactorizer\n",
        "        lk = ensure_object(lk)\n",
        "        rk = ensure_object(rk)\n",
        "\n",
        "    rizer = klass(max(len(lk), len(rk)))\n",
        "\n",
        "    # Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\n",
        "    # \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\n",
        "    # ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\n",
        "    llab = rizer.factorize(lk)  # type: ignore[arg-type]\n",
        "    # Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\n",
        "    # \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\n",
        "    # ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\n",
        "    rlab = rizer.factorize(rk)  # type: ignore[arg-type]\n",
        "    assert llab.dtype == np.dtype(np.intp), llab.dtype\n",
        "    assert rlab.dtype == np.dtype(np.intp), rlab.dtype\n",
        "\n",
        "    count = rizer.get_count()\n",
        "\n",
        "    if sort:\n",
        "        uniques = rizer.uniques.to_array()\n",
        "        llab, rlab = _sort_labels(uniques, llab, rlab)\n",
        "\n",
        "    # NA group\n",
        "    lmask = llab == -1\n",
        "    lany = lmask.any()\n",
        "    rmask = rlab == -1\n",
        "    rany = rmask.any()\n",
        "\n",
        "    if lany or rany:\n",
        "        if lany:\n",
        "            np.putmask(llab, lmask, count)\n",
        "        if rany:\n",
        "            np.putmask(rlab, rmask, count)\n",
        "        count += 1\n",
        "\n",
        "    if how == \"right\":\n",
        "        return rlab, llab, count\n",
        "    return llab, rlab, count"
      ],
      "metadata": {
        "id": "s7pP-tK6YXHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sort_labels(\n",
        "    uniques: np.ndarray, left: npt.NDArray[np.intp], right: npt.NDArray[np.intp]\n",
        ") -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n",
        "\n",
        "    llength = len(left)\n",
        "    labels = np.concatenate([left, right])\n",
        "\n",
        "    _, new_labels = algos.safe_sort(uniques, labels, na_sentinel=-1)\n",
        "    new_left, new_right = new_labels[:llength], new_labels[llength:]\n",
        "\n",
        "    return new_left, new_right\n",
        "\n",
        "\n",
        "def _get_join_keys(llab, rlab, shape, sort: bool):\n",
        "\n",
        "    # how many levels can be done without overflow\n",
        "    nlev = next(\n",
        "        lev\n",
        "        for lev in range(len(shape), 0, -1)\n",
        "        if not is_int64_overflow_possible(shape[:lev])\n",
        "    )\n",
        "\n",
        "    # get keys for the first `nlev` levels\n",
        "    stride = np.prod(shape[1:nlev], dtype=\"i8\")\n",
        "    lkey = stride * llab[0].astype(\"i8\", subok=False, copy=False)\n",
        "    rkey = stride * rlab[0].astype(\"i8\", subok=False, copy=False)\n",
        "\n",
        "    for i in range(1, nlev):\n",
        "        with np.errstate(divide=\"ignore\"):\n",
        "            stride //= shape[i]\n",
        "        lkey += llab[i] * stride\n",
        "        rkey += rlab[i] * stride\n",
        "\n",
        "    if nlev == len(shape):  # all done!\n",
        "        return lkey, rkey\n",
        "\n",
        "    # densify current keys to avoid overflow\n",
        "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)\n",
        "\n",
        "    llab = [lkey] + llab[nlev:]\n",
        "    rlab = [rkey] + rlab[nlev:]\n",
        "    shape = [count] + shape[nlev:]\n",
        "\n",
        "    return _get_join_keys(llab, rlab, shape, sort)\n",
        "\n"
      ],
      "metadata": {
        "id": "WAxZPNNMkRK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _should_fill(lname, rname) -> bool:\n",
        "    if not isinstance(lname, str) or not isinstance(rname, str):\n",
        "        return True\n",
        "    return lname == rname\n",
        "\n",
        "\n",
        "def _any(x) -> bool:\n",
        "    return x is not None and com.any_not_none(*x)\n",
        "\n",
        "\n",
        "def _validate_operand(obj: DataFrame | Series) -> DataFrame:\n",
        "    if isinstance(obj, ABCDataFrame):\n",
        "        return obj\n",
        "    elif isinstance(obj, ABCSeries):\n",
        "        if obj.name is None:\n",
        "            raise ValueError(\"Cannot merge a Series without a name\")\n",
        "        else:\n",
        "            return obj.to_frame()\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f\"Can only merge Series or DataFrame objects, a {type(obj)} was passed\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _items_overlap_with_suffix(\n",
        "    left: Index, right: Index, suffixes: Suffixes\n",
        ") -> tuple[Index, Index]:\n",
        "    \"\"\"\n",
        "    Suffixes type validation.\n",
        "    If two indices overlap, add suffixes to overlapping entries.\n",
        "    If corresponding suffix is empty, the entry is simply converted to string.\n",
        "    \"\"\"\n",
        "    if not is_list_like(suffixes, allow_sets=False):\n",
        "        warnings.warn(\n",
        "            f\"Passing 'suffixes' as a {type(suffixes)}, is not supported and may give \"\n",
        "            \"unexpected results. Provide 'suffixes' as a tuple instead. In the \"\n",
        "            \"future a 'TypeError' will be raised.\",\n",
        "            FutureWarning,\n",
        "            stacklevel=find_stack_level(),\n",
        "        )\n",
        "\n",
        "    to_rename = left.intersection(right)\n",
        "    if len(to_rename) == 0:\n",
        "        return left, right\n",
        "\n",
        "    lsuffix, rsuffix = suffixes\n",
        "\n",
        "    if not lsuffix and not rsuffix:\n",
        "        raise ValueError(f\"columns overlap but no suffix specified: {to_rename}\")\n",
        "\n",
        "    def renamer(x, suffix):\n",
        "        \"\"\"\n",
        "        Rename the left and right indices.\n",
        "        If there is overlap, and suffix is not None, add\n",
        "        suffix, otherwise, leave it as-is.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : original column name\n",
        "        suffix : str or None\n",
        "        Returns\n",
        "        -------\n",
        "        x : renamed column name\n",
        "        \"\"\"\n",
        "        if x in to_rename and suffix is not None:\n",
        "            return f\"{x}{suffix}\"\n",
        "        return x\n",
        "\n",
        "    lrenamer = partial(renamer, suffix=lsuffix)\n",
        "    rrenamer = partial(renamer, suffix=rsuffix)\n",
        "\n",
        "    llabels = left._transform_index(lrenamer)\n",
        "    rlabels = right._transform_index(rrenamer)\n",
        "\n",
        "    dups = []\n",
        "    if not llabels.is_unique:\n",
        "        # Only warn when duplicates are caused because of suffixes, already duplicated\n",
        "        # columns in origin should not warn\n",
        "        dups = llabels[(llabels.duplicated()) & (~left.duplicated())].tolist()\n",
        "    if not rlabels.is_unique:\n",
        "        dups.extend(rlabels[(rlabels.duplicated()) & (~right.duplicated())].tolist())\n",
        "    if dups:\n",
        "        warnings.warn(\n",
        "            f\"Passing 'suffixes' which cause duplicate columns {set(dups)} in the \"\n",
        "            f\"result is deprecated and will raise a MergeError in a future version.\",\n",
        "            FutureWarning,\n",
        "            stacklevel=find_stack_level(),\n",
        "        )\n",
        "\n",
        "    return llabels, rlabels"
      ],
      "metadata": {
        "id": "si3YkEQ7kRIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j9GtS3kGYXKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}